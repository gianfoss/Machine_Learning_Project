{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abJ8gJu4dE0t"
   },
   "source": [
    "# MsCA 31009 - Machine Learning and Predictive Analytics\n",
    "\n",
    "## Project - Toxic Comment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07AdlzCZvP7-"
   },
   "source": [
    "## Import files and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "colab_type": "code",
    "id": "1VTgVkxqTgcm",
    "outputId": "ec8821ab-d250-406a-83ba-093686e010c7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:08:32.584649Z",
     "start_time": "2018-11-10T21:08:25.856600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Y1LX4wIf40NN",
    "outputId": "98e6a84f-0765-4055-823f-f281a5e3ab6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/targoon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/targoon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from autocorrect import spell\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q8krOKh04jgz"
   },
   "source": [
    "**Download train data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:02:43.686849Z",
     "start_time": "2018-11-03T15:02:36.704248Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8UzvEF_u57wM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-03 10:02:36--  https://drive.google.com/uc?export=download&id=1hcoewV5fpD0kx8ysZsZi8EnSjxIgC0lp\n",
      "Resolving drive.google.com (drive.google.com)... 172.217.4.206, 2607:f8b0:4009:807::200e\n",
      "Connecting to drive.google.com (drive.google.com)|172.217.4.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-00-4c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6ua373f1v9qobh534h6f5ape3ahf82pm/1541253600000/00285997938321528797/*/1hcoewV5fpD0kx8ysZsZi8EnSjxIgC0lp?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2018-11-03 10:02:39--  https://doc-00-4c-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/6ua373f1v9qobh534h6f5ape3ahf82pm/1541253600000/00285997938321528797/*/1hcoewV5fpD0kx8ysZsZi8EnSjxIgC0lp?e=download\n",
      "Resolving doc-00-4c-docs.googleusercontent.com (doc-00-4c-docs.googleusercontent.com)... 172.217.5.1, 2607:f8b0:4009:806::2001\n",
      "Connecting to doc-00-4c-docs.googleusercontent.com (doc-00-4c-docs.googleusercontent.com)|172.217.5.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘uc?export=download&id=1hcoewV5fpD0kx8ysZsZi8EnSjxIgC0lp’\n",
      "\n",
      "uc?export=download&     [                 <=>]  26.75M  8.06MB/s    in 3.4s    \n",
      "\n",
      "2018-11-03 10:02:42 (7.93 MB/s) - ‘uc?export=download&id=1hcoewV5fpD0kx8ysZsZi8EnSjxIgC0lp’ saved [28044826]\n",
      "\n",
      "Archive:  uc?export=download&id=1hcoewV5fpD0kx8ysZsZi8EnSjxIgC0lp\n",
      "  inflating: train.csv               \n"
     ]
    }
   ],
   "source": [
    "!wget 'https://drive.google.com/uc?export=download&id=1hcoewV5fpD0kx8ysZsZi8EnSjxIgC0lp'\n",
    "!unzip -o 'uc?export=download&id=1hcoewV5fpD0kx8ysZsZi8EnSjxIgC0lp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:02:45.835034Z",
     "start_time": "2018-11-03T15:02:44.946390Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KxNDXLI32W2D"
   },
   "outputs": [],
   "source": [
    "toxic = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X01706nU2ZKZ"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "colab_type": "code",
    "id": "EEr-AesYK9Cc",
    "outputId": "8225b934-c1b1-4cdd-dc30-e2217a7623b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndkxIDITzFkM"
   },
   "source": [
    "**Remove ID column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:02:55.337773Z",
     "start_time": "2018-11-03T15:02:55.311452Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "6c1PbWjYzElT"
   },
   "outputs": [],
   "source": [
    "toxic.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbQtFVvvzuh5"
   },
   "source": [
    "**Remove non-alphabet characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:02:59.725214Z",
     "start_time": "2018-11-03T15:02:56.882877Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "wWbO4_X_M9Mn"
   },
   "outputs": [],
   "source": [
    "toxic['comment_text'] = [re.sub('[^A-Za-z]', ' ', i).lower() for i in toxic['comment_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R0uDtxCsz-d2"
   },
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:03:46.644100Z",
     "start_time": "2018-11-03T15:03:00.670428Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "uiqXlNpvNeYT",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cb8f2e22f1fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoxic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text_tokenize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoxic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-cb8f2e22f1fe>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoxic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text_tokenize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoxic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Handles parentheses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "toxic['comment_text_tokenize'] = [word_tokenize(i) for i in toxic['comment_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T15:03:46.647450Z",
     "start_time": "2018-11-03T15:03:06.055Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "yFPqb898VJiM",
    "outputId": "7a22a2c4-1429-4f29-ce58-01f8cbc1b0dc"
   },
   "outputs": [],
   "source": [
    "toxic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPIZm7nh0FDg"
   },
   "source": [
    "**Standardize contraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBbBzsl2WWfI"
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    print(confusion_matrix_test_cv_all[i])def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"cant\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dX38Bl6f0dD_"
   },
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLzuow-KOUwQ"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stentence_placeholder = []\n",
    "for sentence in toxic.loc[:,'comment_text_tokenize']:\n",
    "    sentence_stemmed = [stemmer.stem(clean_text(word)) for word in sentence]\n",
    "    stentence_placeholder.append(sentence_stemmed)\n",
    "toxic['comment_text_tokenize_stemmed'] = stentence_placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ciigvcy00gSV"
   },
   "source": [
    "**Stopwords Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhuRuVEoVcQk"
   },
   "outputs": [],
   "source": [
    "stentence_placeholder = []\n",
    "for sentence in toxic.loc[:,'comment_text_tokenize_stemmed']:\n",
    "    sentence_clean = [word for word in sentence if word not in stopwords.words('english')]\n",
    "    stentence_placeholder.append(sentence_clean)\n",
    "toxic['comment_text_clean'] = stentence_placeholder\n",
    "toxic['comment_text_clean'] = [' '.join(i) for i in toxic['comment_text_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "mX3Rx3ZQdVks",
    "outputId": "f4e39b45-9811-4814-9e98-9b228ae1e7c2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_tokenize</th>\n",
       "      <th>comment_text_tokenize_stemmed</th>\n",
       "      <th>comment_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[explan, whi, the, edit, made, under, my, user...</td>\n",
       "      <td>explan whi edit made usernam hardcor metallica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d aww  he matches this background colour i m s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[d, aww, he, matches, this, background, colour...</td>\n",
       "      <td>[d, aww, he, match, this, background, colour, ...</td>\n",
       "      <td>aww match background colour seem stuck thank t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man  i m really not trying to edit war  it...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, i, m, really, not, trying, to, edit...</td>\n",
       "      <td>[hey, man, i, m, realli, not, tri, to, edit, w...</td>\n",
       "      <td>hey man realli tri edit war guy constant remov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more i can t make any real suggestions on im...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[more, i, can, t, make, any, real, suggestions...</td>\n",
       "      <td>[more, i, can, t, make, ani, real, suggest, on...</td>\n",
       "      <td>make ani real suggest improv wonder section st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you  sir  are my hero  any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>[you, sir, are, my, hero, ani, chanc, you, rem...</td>\n",
       "      <td>sir hero ani chanc rememb page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>congratulations from me as well  use the to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[congratulations, from, me, as, well, use, the...</td>\n",
       "      <td>[congratul, from, me, as, well, use, the, tool...</td>\n",
       "      <td>congratul well use tool well talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[cocksucker, before, you, piss, around, on, my...</td>\n",
       "      <td>[cocksuck, befor, you, piss, around, on, my, w...</td>\n",
       "      <td>cocksuck befor piss around work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>your vandalism to the matt shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[your, vandalism, to, the, matt, shirvington, ...</td>\n",
       "      <td>[your, vandal, to, the, matt, shirvington, art...</td>\n",
       "      <td>vandal matt shirvington articl revert pleas ban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sorry if the word  nonsense  was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sorry, if, the, word, nonsense, was, offensiv...</td>\n",
       "      <td>[sorri, if, the, word, nonsens, was, offens, t...</td>\n",
       "      <td>sorri word nonsens offens anyway intend write ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[alignment, on, this, subject, and, which, are...</td>\n",
       "      <td>[align, on, this, subject, and, which, are, co...</td>\n",
       "      <td>align subject contrari dulithgow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fair use rationale for image wonju jpg  than...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[fair, use, rationale, for, image, wonju, jpg,...</td>\n",
       "      <td>[fair, use, rational, for, imag, wonju, jpg, t...</td>\n",
       "      <td>fair use rational imag wonju jpg thank upload ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bbq   be a man and lets discuss it maybe over ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[bbq, be, a, man, and, lets, discuss, it, mayb...</td>\n",
       "      <td>[bbq, be, a, man, and, let, discuss, it, mayb,...</td>\n",
       "      <td>bbq man let discuss mayb phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hey    what is it       talk   what is it    a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, what, is, it, talk, what, is, it, an, ex...</td>\n",
       "      <td>[hey, what, is, it, talk, what, is, it, an, ex...</td>\n",
       "      <td>hey talk exclus group wp taliban good destroy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>before you start throwing accusations and warn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[before, you, start, throwing, accusations, an...</td>\n",
       "      <td>[befor, you, start, throw, accus, and, warn, a...</td>\n",
       "      <td>befor start throw accus warn let review edit m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>oh  and the girl above started her arguments w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[oh, and, the, girl, above, started, her, argu...</td>\n",
       "      <td>[oh, and, the, girl, abov, start, her, argumen...</td>\n",
       "      <td>oh girl abov start argument stuck nose belong ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>juelz santanas age  in       juelz santana ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[juelz, santanas, age, in, juelz, santana, was...</td>\n",
       "      <td>[juelz, santana, age, in, juelz, santana, was,...</td>\n",
       "      <td>juelz santana age juelz santana year old came ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bye    don t look  come or think of comming ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[bye, don, t, look, come, or, think, of, commi...</td>\n",
       "      <td>[bye, don, t, look, come, or, think, of, com, ...</td>\n",
       "      <td>bye look come think com back tosser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>redirect talk voydan pop georgiev  chernodrinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[redirect, talk, voydan, pop, georgiev, cherno...</td>\n",
       "      <td>[redirect, talk, voydan, pop, georgiev, cherno...</td>\n",
       "      <td>redirect talk voydan pop georgiev chernodrinski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the mitsurugi point made no sense   why not ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, mitsurugi, point, made, no, sense, why, ...</td>\n",
       "      <td>[the, mitsurugi, point, made, no, sens, whi, n...</td>\n",
       "      <td>mitsurugi point made sens whi argu includ hind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>don t mean to bother you   i see that you re w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[don, t, mean, to, bother, you, i, see, that, ...</td>\n",
       "      <td>[don, t, mean, to, bother, you, i, see, that, ...</td>\n",
       "      <td>mean bother see write someth regard remov anyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>regarding your recent edits   once again  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[regarding, your, recent, edits, once, again, ...</td>\n",
       "      <td>[regard, your, recent, edit, onc, again, pleas...</td>\n",
       "      <td>regard recent edit onc pleas read wp filmplot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>good to know  about me  yeah  i m studying n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[good, to, know, about, me, yeah, i, m, studyi...</td>\n",
       "      <td>[good, to, know, about, me, yeah, i, m, studi,...</td>\n",
       "      <td>good know yeah studi deepu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>snowflakes are not always symmetrical    u...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[snowflakes, are, not, always, symmetrical, un...</td>\n",
       "      <td>[snowflak, are, not, alway, symmetr, under, ge...</td>\n",
       "      <td>snowflak alway symmetr geometri state snowflak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>the signpost     september         read th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, signpost, september, read, this, signpos...</td>\n",
       "      <td>[the, signpost, septemb, read, this, signpost,...</td>\n",
       "      <td>signpost septemb read signpost full singl page...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>re considering  st paragraph edit  i don t ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[re, considering, st, paragraph, edit, i, don,...</td>\n",
       "      <td>[re, consid, st, paragraph, edit, i, don, t, u...</td>\n",
       "      <td>consid st paragraph edit understand reason rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>radial symmetry   several now extinct lineages...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[radial, symmetry, several, now, extinct, line...</td>\n",
       "      <td>[radial, symmetri, sever, now, extinct, lineag...</td>\n",
       "      <td>radial symmetri sever extinct lineag includ ec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>there s no need to apologize  a wikipedia arti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[there, s, no, need, to, apologize, a, wikiped...</td>\n",
       "      <td>[there, s, no, need, to, apolog, a, wikipedia,...</td>\n",
       "      <td>need apolog wikipedia articl made reconcil kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>yes  because the mother of the child in the ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[yes, because, the, mother, of, the, child, in...</td>\n",
       "      <td>[yes, becaus, the, mother, of, the, child, in,...</td>\n",
       "      <td>yes becaus mother child case michael jackson s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ok  but it will take a bit of work but i can...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ok, but, it, will, take, a, bit, of, work, bu...</td>\n",
       "      <td>[ok, but, it, will, take, a, bit, of, work, bu...</td>\n",
       "      <td>ok take bit work quit pictur exampl base duck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>a barnstar for you        the real life ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[a, barnstar, for, you, the, real, life, barns...</td>\n",
       "      <td>[a, barnstar, for, you, the, real, life, barns...</td>\n",
       "      <td>barnstar real life barnstar let us star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159541</th>\n",
       "      <td>your absurd edits   your absurd edits on great...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[your, absurd, edits, your, absurd, edits, on,...</td>\n",
       "      <td>[your, absurd, edit, your, absurd, edit, on, g...</td>\n",
       "      <td>absurd edit absurd edit great white shark tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159542</th>\n",
       "      <td>maybe he s got better things to do than spend ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[maybe, he, s, got, better, things, to, do, th...</td>\n",
       "      <td>[mayb, he, s, got, better, thing, to, do, than...</td>\n",
       "      <td>mayb got better thing spend day everi day wiki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159543</th>\n",
       "      <td>scrap that  it does meet criteria and its gone...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[scrap, that, it, does, meet, criteria, and, i...</td>\n",
       "      <td>[scrap, that, it, doe, meet, criteria, and, it...</td>\n",
       "      <td>scrap doe meet criteria gone delet review cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159544</th>\n",
       "      <td>you could do worse</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[you, could, do, worse]</td>\n",
       "      <td>[you, could, do, wors]</td>\n",
       "      <td>could wors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159545</th>\n",
       "      <td>march       utc  are you also user bmattso...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[march, utc, are, you, also, user, bmattson, w...</td>\n",
       "      <td>[march, utc, are, you, also, user, bmattson, w...</td>\n",
       "      <td>march utc also user bmattson webaddress discus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159546</th>\n",
       "      <td>hey listen don t you ever     delete my edi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, listen, don, t, you, ever, delete, my, e...</td>\n",
       "      <td>[hey, listen, don, t, you, ever, delet, my, ed...</td>\n",
       "      <td>hey listen ever delet edit ever annoy becaus w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159547</th>\n",
       "      <td>thank you very  very much</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[thank, you, very, very, much]</td>\n",
       "      <td>[thank, you, veri, veri, much]</td>\n",
       "      <td>thank veri veri much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159548</th>\n",
       "      <td>talkback     september</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[talkback, september]</td>\n",
       "      <td>[talkback, septemb]</td>\n",
       "      <td>talkback septemb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159549</th>\n",
       "      <td>utc             mar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[utc, mar]</td>\n",
       "      <td>[utc, mar]</td>\n",
       "      <td>utc mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159550</th>\n",
       "      <td>i agree  on another note lil wayne is a talent...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, agree, on, another, note, lil, wayne, is, ...</td>\n",
       "      <td>[i, agre, on, anoth, note, lil, wayn, is, a, t...</td>\n",
       "      <td>agre anoth note lil wayn talentless man tupac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159551</th>\n",
       "      <td>while about half the references are from byu i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[while, about, half, the, references, are, fro...</td>\n",
       "      <td>[while, about, half, the, refer, are, from, by...</td>\n",
       "      <td>half refer byu major come independ student pap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159552</th>\n",
       "      <td>prague spring   i think that prague spring des...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[prague, spring, i, think, that, prague, sprin...</td>\n",
       "      <td>[pragu, spring, i, think, that, pragu, spring,...</td>\n",
       "      <td>pragu spring think pragu spring deserv two sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159553</th>\n",
       "      <td>i see this as having been merged  undoing one ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, see, this, as, having, been, merged, undoi...</td>\n",
       "      <td>[i, see, this, as, have, been, merg, undo, one...</td>\n",
       "      <td>see merg undo one side merg fork content larg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159554</th>\n",
       "      <td>and i m going to keep posting the stuff u dele...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[and, i, m, going, to, keep, posting, the, stu...</td>\n",
       "      <td>[and, i, m, go, to, keep, post, the, stuff, u,...</td>\n",
       "      <td>go keep post stuff u delet fuck site close fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159555</th>\n",
       "      <td>how come when you download that mp  it s ti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[how, come, when, you, download, that, mp, it,...</td>\n",
       "      <td>[how, come, when, you, download, that, mp, it,...</td>\n",
       "      <td>come download mp titl odb odb theme point along</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159556</th>\n",
       "      <td>i ll be on irc  too  if you have a more specif...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, ll, be, on, irc, too, if, you, have, a, mo...</td>\n",
       "      <td>[i, ll, be, on, irc, too, if, you, have, a, mo...</td>\n",
       "      <td>irc specif request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159557</th>\n",
       "      <td>it is my opinion that that happens to be off t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[it, is, my, opinion, that, that, happens, to,...</td>\n",
       "      <td>[it, is, my, opinion, that, that, happen, to, ...</td>\n",
       "      <td>opinion happen topic believ never claim common...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159558</th>\n",
       "      <td>please stop removing content from wikipedia  i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[please, stop, removing, content, from, wikipe...</td>\n",
       "      <td>[pleas, stop, remov, content, from, wikipedia,...</td>\n",
       "      <td>pleas stop remov content wikipedia consid vand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159559</th>\n",
       "      <td>image barack obama mother jpg listed for delet...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[image, barack, obama, mother, jpg, listed, fo...</td>\n",
       "      <td>[imag, barack, obama, mother, jpg, list, for, ...</td>\n",
       "      <td>imag barack obama mother jpg list delet imag m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159560</th>\n",
       "      <td>editing of article without consensus   remova...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[editing, of, article, without, consensus, rem...</td>\n",
       "      <td>[edit, of, articl, without, consensus, remov, ...</td>\n",
       "      <td>edit articl without consensus remov cite conte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159561</th>\n",
       "      <td>no he did not  read it again  i would have t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[no, he, did, not, read, it, again, i, would, ...</td>\n",
       "      <td>[no, he, did, not, read, it, again, i, would, ...</td>\n",
       "      <td>read would thought everyon could recit heart s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159562</th>\n",
       "      <td>auto guides and the motoring press are not ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[auto, guides, and, the, motoring, press, are,...</td>\n",
       "      <td>[auto, guid, and, the, motor, press, are, not,...</td>\n",
       "      <td>auto guid motor press good sourc encyclopedia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159563</th>\n",
       "      <td>please identify what part of blp applies bec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[please, identify, what, part, of, blp, applie...</td>\n",
       "      <td>[pleas, identifi, what, part, of, blp, appli, ...</td>\n",
       "      <td>pleas identifi part blp appli becaus blp clear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159564</th>\n",
       "      <td>catalan independentism is the social movement ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[catalan, independentism, is, the, social, mov...</td>\n",
       "      <td>[catalan, independent, is, the, social, moveme...</td>\n",
       "      <td>catalan independent social movement involv peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159565</th>\n",
       "      <td>the numbers in parentheses are the additional ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, numbers, in, parentheses, are, the, addi...</td>\n",
       "      <td>[the, number, in, parenthes, are, the, addit, ...</td>\n",
       "      <td>number parenthes addit decim point measur lie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>and for the second time of asking  when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[and, for, the, second, time, of, asking, when...</td>\n",
       "      <td>[and, for, the, second, time, of, ask, when, y...</td>\n",
       "      <td>second time ask view complet contradict covera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>you should be ashamed of yourself   that is a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[you, should, be, ashamed, of, yourself, that,...</td>\n",
       "      <td>[you, should, be, asham, of, yourself, that, i...</td>\n",
       "      <td>asham horribl thing put talk page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>spitzer   umm  theres no actual article for pr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[spitzer, umm, theres, no, actual, article, fo...</td>\n",
       "      <td>[spitzer, umm, there, no, actual, articl, for,...</td>\n",
       "      <td>spitzer umm actual articl prostitut ring crunc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[and, it, looks, like, it, was, actually, you,...</td>\n",
       "      <td>[and, it, look, like, it, was, actual, you, wh...</td>\n",
       "      <td>look like actual put speedi first version dele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>and     i really don t think you understand ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[and, i, really, don, t, think, you, understan...</td>\n",
       "      <td>[and, i, realli, don, t, think, you, understan...</td>\n",
       "      <td>realli think understand came idea bad right aw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic  \\\n",
       "0       explanation why the edits made under my userna...      0   \n",
       "1       d aww  he matches this background colour i m s...      0   \n",
       "2       hey man  i m really not trying to edit war  it...      0   \n",
       "3         more i can t make any real suggestions on im...      0   \n",
       "4       you  sir  are my hero  any chance you remember...      0   \n",
       "5          congratulations from me as well  use the to...      0   \n",
       "6            cocksucker before you piss around on my work      1   \n",
       "7       your vandalism to the matt shirvington article...      0   \n",
       "8       sorry if the word  nonsense  was offensive to ...      0   \n",
       "9       alignment on this subject and which are contra...      0   \n",
       "10        fair use rationale for image wonju jpg  than...      0   \n",
       "11      bbq   be a man and lets discuss it maybe over ...      0   \n",
       "12      hey    what is it       talk   what is it    a...      1   \n",
       "13      before you start throwing accusations and warn...      0   \n",
       "14      oh  and the girl above started her arguments w...      0   \n",
       "15         juelz santanas age  in       juelz santana ...      0   \n",
       "16      bye    don t look  come or think of comming ba...      1   \n",
       "17       redirect talk voydan pop georgiev  chernodrinski      0   \n",
       "18      the mitsurugi point made no sense   why not ar...      0   \n",
       "19      don t mean to bother you   i see that you re w...      0   \n",
       "20          regarding your recent edits   once again  ...      0   \n",
       "21        good to know  about me  yeah  i m studying n...      0   \n",
       "22          snowflakes are not always symmetrical    u...      0   \n",
       "23          the signpost     september         read th...      0   \n",
       "24         re considering  st paragraph edit  i don t ...      0   \n",
       "25      radial symmetry   several now extinct lineages...      0   \n",
       "26      there s no need to apologize  a wikipedia arti...      0   \n",
       "27      yes  because the mother of the child in the ca...      0   \n",
       "28        ok  but it will take a bit of work but i can...      0   \n",
       "29          a barnstar for you        the real life ba...      0   \n",
       "...                                                   ...    ...   \n",
       "159541  your absurd edits   your absurd edits on great...      1   \n",
       "159542  maybe he s got better things to do than spend ...      0   \n",
       "159543  scrap that  it does meet criteria and its gone...      0   \n",
       "159544                                you could do worse       0   \n",
       "159545      march       utc  are you also user bmattso...      0   \n",
       "159546     hey listen don t you ever     delete my edi...      1   \n",
       "159547                     thank you very  very much           0   \n",
       "159548                        talkback     september           0   \n",
       "159549                                utc             mar      0   \n",
       "159550  i agree  on another note lil wayne is a talent...      0   \n",
       "159551  while about half the references are from byu i...      0   \n",
       "159552  prague spring   i think that prague spring des...      0   \n",
       "159553  i see this as having been merged  undoing one ...      0   \n",
       "159554  and i m going to keep posting the stuff u dele...      1   \n",
       "159555     how come when you download that mp  it s ti...      0   \n",
       "159556  i ll be on irc  too  if you have a more specif...      0   \n",
       "159557  it is my opinion that that happens to be off t...      0   \n",
       "159558  please stop removing content from wikipedia  i...      0   \n",
       "159559  image barack obama mother jpg listed for delet...      0   \n",
       "159560   editing of article without consensus   remova...      0   \n",
       "159561    no he did not  read it again  i would have t...      0   \n",
       "159562     auto guides and the motoring press are not ...      0   \n",
       "159563    please identify what part of blp applies bec...      0   \n",
       "159564  catalan independentism is the social movement ...      0   \n",
       "159565  the numbers in parentheses are the additional ...      0   \n",
       "159566        and for the second time of asking  when ...      0   \n",
       "159567  you should be ashamed of yourself   that is a ...      0   \n",
       "159568  spitzer   umm  theres no actual article for pr...      0   \n",
       "159569  and it looks like it was actually you who put ...      0   \n",
       "159570    and     i really don t think you understand ...      0   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0                  0        0       0       0              0   \n",
       "1                  0        0       0       0              0   \n",
       "2                  0        0       0       0              0   \n",
       "3                  0        0       0       0              0   \n",
       "4                  0        0       0       0              0   \n",
       "5                  0        0       0       0              0   \n",
       "6                  1        1       0       1              0   \n",
       "7                  0        0       0       0              0   \n",
       "8                  0        0       0       0              0   \n",
       "9                  0        0       0       0              0   \n",
       "10                 0        0       0       0              0   \n",
       "11                 0        0       0       0              0   \n",
       "12                 0        0       0       0              0   \n",
       "13                 0        0       0       0              0   \n",
       "14                 0        0       0       0              0   \n",
       "15                 0        0       0       0              0   \n",
       "16                 0        0       0       0              0   \n",
       "17                 0        0       0       0              0   \n",
       "18                 0        0       0       0              0   \n",
       "19                 0        0       0       0              0   \n",
       "20                 0        0       0       0              0   \n",
       "21                 0        0       0       0              0   \n",
       "22                 0        0       0       0              0   \n",
       "23                 0        0       0       0              0   \n",
       "24                 0        0       0       0              0   \n",
       "25                 0        0       0       0              0   \n",
       "26                 0        0       0       0              0   \n",
       "27                 0        0       0       0              0   \n",
       "28                 0        0       0       0              0   \n",
       "29                 0        0       0       0              0   \n",
       "...              ...      ...     ...     ...            ...   \n",
       "159541             0        1       0       1              0   \n",
       "159542             0        0       0       0              0   \n",
       "159543             0        0       0       0              0   \n",
       "159544             0        0       0       0              0   \n",
       "159545             0        0       0       0              0   \n",
       "159546             0        0       0       1              0   \n",
       "159547             0        0       0       0              0   \n",
       "159548             0        0       0       0              0   \n",
       "159549             0        0       0       0              0   \n",
       "159550             0        0       0       0              0   \n",
       "159551             0        0       0       0              0   \n",
       "159552             0        0       0       0              0   \n",
       "159553             0        0       0       0              0   \n",
       "159554             0        1       0       1              0   \n",
       "159555             0        0       0       0              0   \n",
       "159556             0        0       0       0              0   \n",
       "159557             0        0       0       0              0   \n",
       "159558             0        0       0       0              0   \n",
       "159559             0        0       0       0              0   \n",
       "159560             0        0       0       0              0   \n",
       "159561             0        0       0       0              0   \n",
       "159562             0        0       0       0              0   \n",
       "159563             0        0       0       0              0   \n",
       "159564             0        0       0       0              0   \n",
       "159565             0        0       0       0              0   \n",
       "159566             0        0       0       0              0   \n",
       "159567             0        0       0       0              0   \n",
       "159568             0        0       0       0              0   \n",
       "159569             0        0       0       0              0   \n",
       "159570             0        0       0       0              0   \n",
       "\n",
       "                                    comment_text_tokenize  \\\n",
       "0       [explanation, why, the, edits, made, under, my...   \n",
       "1       [d, aww, he, matches, this, background, colour...   \n",
       "2       [hey, man, i, m, really, not, trying, to, edit...   \n",
       "3       [more, i, can, t, make, any, real, suggestions...   \n",
       "4       [you, sir, are, my, hero, any, chance, you, re...   \n",
       "5       [congratulations, from, me, as, well, use, the...   \n",
       "6       [cocksucker, before, you, piss, around, on, my...   \n",
       "7       [your, vandalism, to, the, matt, shirvington, ...   \n",
       "8       [sorry, if, the, word, nonsense, was, offensiv...   \n",
       "9       [alignment, on, this, subject, and, which, are...   \n",
       "10      [fair, use, rationale, for, image, wonju, jpg,...   \n",
       "11      [bbq, be, a, man, and, lets, discuss, it, mayb...   \n",
       "12      [hey, what, is, it, talk, what, is, it, an, ex...   \n",
       "13      [before, you, start, throwing, accusations, an...   \n",
       "14      [oh, and, the, girl, above, started, her, argu...   \n",
       "15      [juelz, santanas, age, in, juelz, santana, was...   \n",
       "16      [bye, don, t, look, come, or, think, of, commi...   \n",
       "17      [redirect, talk, voydan, pop, georgiev, cherno...   \n",
       "18      [the, mitsurugi, point, made, no, sense, why, ...   \n",
       "19      [don, t, mean, to, bother, you, i, see, that, ...   \n",
       "20      [regarding, your, recent, edits, once, again, ...   \n",
       "21      [good, to, know, about, me, yeah, i, m, studyi...   \n",
       "22      [snowflakes, are, not, always, symmetrical, un...   \n",
       "23      [the, signpost, september, read, this, signpos...   \n",
       "24      [re, considering, st, paragraph, edit, i, don,...   \n",
       "25      [radial, symmetry, several, now, extinct, line...   \n",
       "26      [there, s, no, need, to, apologize, a, wikiped...   \n",
       "27      [yes, because, the, mother, of, the, child, in...   \n",
       "28      [ok, but, it, will, take, a, bit, of, work, bu...   \n",
       "29      [a, barnstar, for, you, the, real, life, barns...   \n",
       "...                                                   ...   \n",
       "159541  [your, absurd, edits, your, absurd, edits, on,...   \n",
       "159542  [maybe, he, s, got, better, things, to, do, th...   \n",
       "159543  [scrap, that, it, does, meet, criteria, and, i...   \n",
       "159544                            [you, could, do, worse]   \n",
       "159545  [march, utc, are, you, also, user, bmattson, w...   \n",
       "159546  [hey, listen, don, t, you, ever, delete, my, e...   \n",
       "159547                     [thank, you, very, very, much]   \n",
       "159548                              [talkback, september]   \n",
       "159549                                         [utc, mar]   \n",
       "159550  [i, agree, on, another, note, lil, wayne, is, ...   \n",
       "159551  [while, about, half, the, references, are, fro...   \n",
       "159552  [prague, spring, i, think, that, prague, sprin...   \n",
       "159553  [i, see, this, as, having, been, merged, undoi...   \n",
       "159554  [and, i, m, going, to, keep, posting, the, stu...   \n",
       "159555  [how, come, when, you, download, that, mp, it,...   \n",
       "159556  [i, ll, be, on, irc, too, if, you, have, a, mo...   \n",
       "159557  [it, is, my, opinion, that, that, happens, to,...   \n",
       "159558  [please, stop, removing, content, from, wikipe...   \n",
       "159559  [image, barack, obama, mother, jpg, listed, fo...   \n",
       "159560  [editing, of, article, without, consensus, rem...   \n",
       "159561  [no, he, did, not, read, it, again, i, would, ...   \n",
       "159562  [auto, guides, and, the, motoring, press, are,...   \n",
       "159563  [please, identify, what, part, of, blp, applie...   \n",
       "159564  [catalan, independentism, is, the, social, mov...   \n",
       "159565  [the, numbers, in, parentheses, are, the, addi...   \n",
       "159566  [and, for, the, second, time, of, asking, when...   \n",
       "159567  [you, should, be, ashamed, of, yourself, that,...   \n",
       "159568  [spitzer, umm, theres, no, actual, article, fo...   \n",
       "159569  [and, it, looks, like, it, was, actually, you,...   \n",
       "159570  [and, i, really, don, t, think, you, understan...   \n",
       "\n",
       "                            comment_text_tokenize_stemmed  \\\n",
       "0       [explan, whi, the, edit, made, under, my, user...   \n",
       "1       [d, aww, he, match, this, background, colour, ...   \n",
       "2       [hey, man, i, m, realli, not, tri, to, edit, w...   \n",
       "3       [more, i, can, t, make, ani, real, suggest, on...   \n",
       "4       [you, sir, are, my, hero, ani, chanc, you, rem...   \n",
       "5       [congratul, from, me, as, well, use, the, tool...   \n",
       "6       [cocksuck, befor, you, piss, around, on, my, w...   \n",
       "7       [your, vandal, to, the, matt, shirvington, art...   \n",
       "8       [sorri, if, the, word, nonsens, was, offens, t...   \n",
       "9       [align, on, this, subject, and, which, are, co...   \n",
       "10      [fair, use, rational, for, imag, wonju, jpg, t...   \n",
       "11      [bbq, be, a, man, and, let, discuss, it, mayb,...   \n",
       "12      [hey, what, is, it, talk, what, is, it, an, ex...   \n",
       "13      [befor, you, start, throw, accus, and, warn, a...   \n",
       "14      [oh, and, the, girl, abov, start, her, argumen...   \n",
       "15      [juelz, santana, age, in, juelz, santana, was,...   \n",
       "16      [bye, don, t, look, come, or, think, of, com, ...   \n",
       "17      [redirect, talk, voydan, pop, georgiev, cherno...   \n",
       "18      [the, mitsurugi, point, made, no, sens, whi, n...   \n",
       "19      [don, t, mean, to, bother, you, i, see, that, ...   \n",
       "20      [regard, your, recent, edit, onc, again, pleas...   \n",
       "21      [good, to, know, about, me, yeah, i, m, studi,...   \n",
       "22      [snowflak, are, not, alway, symmetr, under, ge...   \n",
       "23      [the, signpost, septemb, read, this, signpost,...   \n",
       "24      [re, consid, st, paragraph, edit, i, don, t, u...   \n",
       "25      [radial, symmetri, sever, now, extinct, lineag...   \n",
       "26      [there, s, no, need, to, apolog, a, wikipedia,...   \n",
       "27      [yes, becaus, the, mother, of, the, child, in,...   \n",
       "28      [ok, but, it, will, take, a, bit, of, work, bu...   \n",
       "29      [a, barnstar, for, you, the, real, life, barns...   \n",
       "...                                                   ...   \n",
       "159541  [your, absurd, edit, your, absurd, edit, on, g...   \n",
       "159542  [mayb, he, s, got, better, thing, to, do, than...   \n",
       "159543  [scrap, that, it, doe, meet, criteria, and, it...   \n",
       "159544                             [you, could, do, wors]   \n",
       "159545  [march, utc, are, you, also, user, bmattson, w...   \n",
       "159546  [hey, listen, don, t, you, ever, delet, my, ed...   \n",
       "159547                     [thank, you, veri, veri, much]   \n",
       "159548                                [talkback, septemb]   \n",
       "159549                                         [utc, mar]   \n",
       "159550  [i, agre, on, anoth, note, lil, wayn, is, a, t...   \n",
       "159551  [while, about, half, the, refer, are, from, by...   \n",
       "159552  [pragu, spring, i, think, that, pragu, spring,...   \n",
       "159553  [i, see, this, as, have, been, merg, undo, one...   \n",
       "159554  [and, i, m, go, to, keep, post, the, stuff, u,...   \n",
       "159555  [how, come, when, you, download, that, mp, it,...   \n",
       "159556  [i, ll, be, on, irc, too, if, you, have, a, mo...   \n",
       "159557  [it, is, my, opinion, that, that, happen, to, ...   \n",
       "159558  [pleas, stop, remov, content, from, wikipedia,...   \n",
       "159559  [imag, barack, obama, mother, jpg, list, for, ...   \n",
       "159560  [edit, of, articl, without, consensus, remov, ...   \n",
       "159561  [no, he, did, not, read, it, again, i, would, ...   \n",
       "159562  [auto, guid, and, the, motor, press, are, not,...   \n",
       "159563  [pleas, identifi, what, part, of, blp, appli, ...   \n",
       "159564  [catalan, independent, is, the, social, moveme...   \n",
       "159565  [the, number, in, parenthes, are, the, addit, ...   \n",
       "159566  [and, for, the, second, time, of, ask, when, y...   \n",
       "159567  [you, should, be, asham, of, yourself, that, i...   \n",
       "159568  [spitzer, umm, there, no, actual, articl, for,...   \n",
       "159569  [and, it, look, like, it, was, actual, you, wh...   \n",
       "159570  [and, i, realli, don, t, think, you, understan...   \n",
       "\n",
       "                                       comment_text_clean  \n",
       "0       explan whi edit made usernam hardcor metallica...  \n",
       "1       aww match background colour seem stuck thank t...  \n",
       "2       hey man realli tri edit war guy constant remov...  \n",
       "3       make ani real suggest improv wonder section st...  \n",
       "4                          sir hero ani chanc rememb page  \n",
       "5                       congratul well use tool well talk  \n",
       "6                         cocksuck befor piss around work  \n",
       "7         vandal matt shirvington articl revert pleas ban  \n",
       "8       sorri word nonsens offens anyway intend write ...  \n",
       "9                        align subject contrari dulithgow  \n",
       "10      fair use rational imag wonju jpg thank upload ...  \n",
       "11                         bbq man let discuss mayb phone  \n",
       "12      hey talk exclus group wp taliban good destroy ...  \n",
       "13      befor start throw accus warn let review edit m...  \n",
       "14      oh girl abov start argument stuck nose belong ...  \n",
       "15      juelz santana age juelz santana year old came ...  \n",
       "16                    bye look come think com back tosser  \n",
       "17        redirect talk voydan pop georgiev chernodrinski  \n",
       "18      mitsurugi point made sens whi argu includ hind...  \n",
       "19      mean bother see write someth regard remov anyt...  \n",
       "20      regard recent edit onc pleas read wp filmplot ...  \n",
       "21                             good know yeah studi deepu  \n",
       "22      snowflak alway symmetr geometri state snowflak...  \n",
       "23      signpost septemb read signpost full singl page...  \n",
       "24      consid st paragraph edit understand reason rec...  \n",
       "25      radial symmetri sever extinct lineag includ ec...  \n",
       "26      need apolog wikipedia articl made reconcil kno...  \n",
       "27      yes becaus mother child case michael jackson s...  \n",
       "28          ok take bit work quit pictur exampl base duck  \n",
       "29                barnstar real life barnstar let us star  \n",
       "...                                                   ...  \n",
       "159541  absurd edit absurd edit great white shark tota...  \n",
       "159542  mayb got better thing spend day everi day wiki...  \n",
       "159543  scrap doe meet criteria gone delet review cont...  \n",
       "159544                                         could wors  \n",
       "159545  march utc also user bmattson webaddress discus...  \n",
       "159546  hey listen ever delet edit ever annoy becaus w...  \n",
       "159547                               thank veri veri much  \n",
       "159548                                   talkback septemb  \n",
       "159549                                            utc mar  \n",
       "159550  agre anoth note lil wayn talentless man tupac ...  \n",
       "159551  half refer byu major come independ student pap...  \n",
       "159552  pragu spring think pragu spring deserv two sen...  \n",
       "159553  see merg undo one side merg fork content larg ...  \n",
       "159554  go keep post stuff u delet fuck site close fun...  \n",
       "159555    come download mp titl odb odb theme point along  \n",
       "159556                                 irc specif request  \n",
       "159557  opinion happen topic believ never claim common...  \n",
       "159558  pleas stop remov content wikipedia consid vand...  \n",
       "159559  imag barack obama mother jpg list delet imag m...  \n",
       "159560  edit articl without consensus remov cite conte...  \n",
       "159561  read would thought everyon could recit heart s...  \n",
       "159562  auto guid motor press good sourc encyclopedia ...  \n",
       "159563  pleas identifi part blp appli becaus blp clear...  \n",
       "159564  catalan independent social movement involv peo...  \n",
       "159565  number parenthes addit decim point measur lie ...  \n",
       "159566  second time ask view complet contradict covera...  \n",
       "159567                  asham horribl thing put talk page  \n",
       "159568  spitzer umm actual articl prostitut ring crunc...  \n",
       "159569  look like actual put speedi first version dele...  \n",
       "159570  realli think understand came idea bad right aw...  \n",
       "\n",
       "[159571 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic.to_csv('train_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2odG7o7L1_vY"
   },
   "source": [
    "### Create feature spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = pd.read_csv('train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop NA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_tokenize</th>\n",
       "      <th>comment_text_tokenize_stemmed</th>\n",
       "      <th>comment_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159521</td>\n",
       "      <td>159521.000000</td>\n",
       "      <td>159521.000000</td>\n",
       "      <td>159521.000000</td>\n",
       "      <td>159521.000000</td>\n",
       "      <td>159521.000000</td>\n",
       "      <td>159521.000000</td>\n",
       "      <td>159521</td>\n",
       "      <td>159521</td>\n",
       "      <td>159521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>159255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158206</td>\n",
       "      <td>158181</td>\n",
       "      <td>157648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>jun       utc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['january']</td>\n",
       "      <td>['januari']</td>\n",
       "      <td>thank experi wikipedia test work revert remov ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.095875</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>0.052965</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049379</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.294420</td>\n",
       "      <td>0.099493</td>\n",
       "      <td>0.223964</td>\n",
       "      <td>0.054658</td>\n",
       "      <td>0.216659</td>\n",
       "      <td>0.093435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    comment_text          toxic   severe_toxic        obscene  \\\n",
       "count                     159521  159521.000000  159521.000000  159521.000000   \n",
       "unique                    159255            NaN            NaN            NaN   \n",
       "top               jun       utc             NaN            NaN            NaN   \n",
       "freq                          11            NaN            NaN            NaN   \n",
       "mean                         NaN       0.095875       0.009999       0.052965   \n",
       "std                          NaN       0.294420       0.099493       0.223964   \n",
       "min                          NaN       0.000000       0.000000       0.000000   \n",
       "25%                          NaN       0.000000       0.000000       0.000000   \n",
       "50%                          NaN       0.000000       0.000000       0.000000   \n",
       "75%                          NaN       0.000000       0.000000       0.000000   \n",
       "max                          NaN       1.000000       1.000000       1.000000   \n",
       "\n",
       "               threat         insult  identity_hate comment_text_tokenize  \\\n",
       "count   159521.000000  159521.000000  159521.000000                159521   \n",
       "unique            NaN            NaN            NaN                158206   \n",
       "top               NaN            NaN            NaN           ['january']   \n",
       "freq              NaN            NaN            NaN                    21   \n",
       "mean         0.002996       0.049379       0.008808                   NaN   \n",
       "std          0.054658       0.216659       0.093435                   NaN   \n",
       "min          0.000000       0.000000       0.000000                   NaN   \n",
       "25%          0.000000       0.000000       0.000000                   NaN   \n",
       "50%          0.000000       0.000000       0.000000                   NaN   \n",
       "75%          0.000000       0.000000       0.000000                   NaN   \n",
       "max          1.000000       1.000000       1.000000                   NaN   \n",
       "\n",
       "       comment_text_tokenize_stemmed  \\\n",
       "count                         159521   \n",
       "unique                        158181   \n",
       "top                      ['januari']   \n",
       "freq                              21   \n",
       "mean                             NaN   \n",
       "std                              NaN   \n",
       "min                              NaN   \n",
       "25%                              NaN   \n",
       "50%                              NaN   \n",
       "75%                              NaN   \n",
       "max                              NaN   \n",
       "\n",
       "                                       comment_text_clean  \n",
       "count                                              159521  \n",
       "unique                                             157648  \n",
       "top     thank experi wikipedia test work revert remov ...  \n",
       "freq                                                   22  \n",
       "mean                                                  NaN  \n",
       "std                                                   NaN  \n",
       "min                                                   NaN  \n",
       "25%                                                   NaN  \n",
       "50%                                                   NaN  \n",
       "75%                                                   NaN  \n",
       "max                                                   NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Train and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(toxic.loc[:,'comment_text_clean'], toxic.iloc[:,1:7], test_size = .3, random_state = 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21524    thank note worri wait period get permiss owner...\n",
       "56229    page need massiv edit initi section befor hit ...\n",
       "93765                                       okaaaaaay test\n",
       "87443    apologis make remark sidaway return perhap cou...\n",
       "73667    newspap headlin newspap headlin blank adult sw...\n",
       "Name: comment_text_clean, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111664,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21524</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56229</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93765</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87443</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73667</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "21524      0             0        0       0       0              0\n",
       "56229      0             0        0       0       0              0\n",
       "93765      0             0        0       0       0              0\n",
       "87443      0             0        0       0       0              0\n",
       "73667      0             0        0       0       0              0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create feature spaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "YfCp-SrAdfvD",
    "outputId": "64f732e0-f6dd-4f7a-82a1-698a250f59d4"
   },
   "outputs": [],
   "source": [
    "#Count Vectors as features\n",
    "\n",
    "count_vect = CountVectorizer(max_features=5000)\n",
    "count_vect.fit(x_train)\n",
    "x_train_cv = count_vect.transform(x_train)\n",
    "x_test_cv = count_vect.transform(x_test)\n",
    "\n",
    "#TF-IDF Vectors as features\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(x_train)\n",
    "x_train_tfidf =  tfidf_vect.transform(x_train)\n",
    "x_test_tfidf =  tfidf_vect.transform(x_test)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(x_train)\n",
    "x_train_tfidf_ngram =  tfidf_vect_ngram.transform(x_train)\n",
    "x_test_tfidf_ngram =  tfidf_vect_ngram.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name_cv = count_vect.get_feature_names()\n",
    "feature_name_tfidf = tfidf_vect.get_feature_names()\n",
    "feature_name_ngram = tfidf_vect_ngram.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaron', 'ab', 'abandon', 'abbrevi', 'abc', 'abid', 'abil', 'abl', 'abort', 'abov', 'abraham', 'abroad', 'absenc', 'absent', 'absolut', 'abstract', 'absurd', 'abund', 'abus', 'ac', 'academ', 'academi', 'acceler', 'accent', 'accept', 'access', 'accid', 'accident', 'accommod', 'accompani', 'accomplish', 'accord', 'account', 'accur', 'accuraci', 'accus', 'ace', 'achiev', 'acid', 'acknowledg', 'acquir', 'acronym', 'across', 'act', 'action', 'activ', 'activist', 'actor', 'actress', 'actual', 'ad', 'adam', 'adapt', 'add', 'addict', 'addit', 'address', 'adequ', 'adher', 'adject', 'adjust', 'admin', 'administ', 'administr', 'adminship', 'admir', 'admiss', 'admit', 'adolf', 'adopt', 'adress', 'adult', 'advanc', 'advantag', 'adventur', 'advert', 'advertis', 'advic', 'advis', 'advoc', 'advocaci', 'ae', 'aesthet', 'afc', 'afd', 'affair', 'affect', 'affili', 'affirm', 'afford', 'afghan', 'afghanistan', 'aforement', 'afraid', 'africa', 'african', 'afternoon', 'afterward', 'age', 'agenc', 'agenda', 'agent', 'agf', 'aggress', 'ago', 'agre', 'agreement', 'agricultur', 'ah', 'ahead', 'ai', 'aid', 'aidsaid', 'aim', 'aint', 'air', 'aircraft', 'airlin', 'airport', 'aiv', 'aka', 'akin', 'al', 'ala', 'alan', 'albania', 'albanian', 'albeit', 'albert', 'album', 'alcohol', 'alert', 'alex', 'alexand', 'algebra', 'algorithm', 'ali', 'alia', 'alien', 'align', 'alik', 'aliv', 'allah', 'alleg', 'allen', 'alli', 'allianc', 'allow', 'almost', 'alon', 'along', 'alongsid', 'alot', 'alpha', 'alphabet', 'alreadi', 'alright', 'also', 'alt', 'alter', 'altern', 'although', 'altogeth', 'alumni', 'alway', 'amateur', 'amaz', 'amazon', 'ambassador', 'ambigu', 'amend', 'america', 'american', 'among', 'amongst', 'amount', 'amus', 'anal', 'analog', 'analys', 'analysi', 'analyz', 'anarchist', 'ancestor', 'ancestri', 'ancestryfuck', 'ancient', 'anderson', 'andi', 'andrew', 'angel', 'angela', 'anger', 'angl', 'anglo', 'angri', 'ani', 'anim', 'ann', 'anna', 'annex', 'anniversari', 'announc', 'annoy', 'annual', 'anon', 'anonym', 'anoth', 'answer', 'ant', 'anthoni', 'anthropolog', 'anti', 'anticip', 'antisemit', 'antivman', 'antonio', 'anybodi', 'anyhow', 'anymor', 'anyon', 'anyth', 'anytim', 'anyway', 'anywher', 'aol', 'ap', 'apart', 'ape', 'apolog', 'apologis', 'appal', 'appar', 'appeal', 'appear', 'appl', 'appli', 'applic', 'appoint', 'appreci', 'approach', 'appropri', 'approv', 'approxim', 'apr', 'april', 'ar', 'arab', 'arabia', 'arb', 'arbcom', 'arbitr', 'arbitrari', 'arbitrarili', 'archaeolog', 'architectur', 'archiv', 'area', 'argentina', 'argu', 'arguabl', 'arguement', 'argument', 'aris', 'arm', 'armenia', 'armenian', 'armi', 'around', 'arrang', 'arrest', 'arriv', 'arrog', 'ars', 'art', 'arthur', 'articl', 'artifici', 'artist', 'aryan', 'asap', 'asham', 'asia', 'asian', 'asid', 'ask', 'asp', 'aspect', 'aspx', 'ass', 'assad', 'assassin', 'assault', 'assembl', 'assert', 'assess', 'asset', 'asshol', 'assign', 'assist', 'associ', 'assum', 'assumpt', 'assur', 'assyrian', 'astrolog', 'astronom', 'atheism', 'atheist', 'athlet', 'atlant', 'atleast', 'atmospher', 'atom', 'attach', 'attack', 'attempt', 'attend', 'attent', 'attest', 'attitud', 'attorney', 'attract', 'attribut', 'au', 'audienc', 'audio', 'aug', 'august', 'austin', 'australia', 'australian', 'austria', 'austrian', 'authent', 'author', 'authorit', 'authoritarian', 'autism', 'auto', 'autobiographi', 'autoblock', 'autom', 'automat', 'avail', 'avenu', 'averag', 'aviat', 'avoid', 'aw', 'await', 'awar', 'award', 'awardhttp', 'away', 'awb', 'awesom', 'awhil', 'awkward', 'axe', 'axi', 'azerbaijan', 'azerbaijani', 'azeri', 'b', 'ba', 'babi', 'babywhat', 'back', 'background', 'backlog', 'backward', 'bad', 'badg', 'bag', 'bailey', 'bait', 'baker', 'balanc', 'balkan', 'ball', 'ban', 'band', 'bang', 'bank', 'banner', 'baptist', 'bar', 'barack', 'bare', 'barnstar', 'barri', 'base', 'basebal', 'baseless', 'basement', 'bash', 'basi', 'basic', 'basketbal', 'basqu', 'bass', 'bastard', 'baster', 'bat', 'batman', 'batteri', 'battl', 'battleground', 'bay', 'bb', 'bbb', 'bbc', 'bc', 'bce', 'beach', 'bear', 'beast', 'beat', 'beatl', 'beauti', 'becam', 'becaus', 'becom', 'becuas', 'bed', 'bee', 'beef', 'beer', 'befor', 'beg', 'began', 'begin', 'begun', 'behalf', 'behav', 'behavior', 'behaviour', 'behind', 'belief', 'believ', 'beliv', 'bell', 'belong', 'belt', 'ben', 'benefici', 'benefit', 'bent', 'berlin', 'berri', 'besid', 'best', 'bestfrozen', 'bet', 'beta', 'betray', 'better', 'beyond', 'bi', 'bias', 'bibl', 'biblic', 'bibliographi', 'bid', 'big', 'bigger', 'biggest', 'bigot', 'bigotri', 'bill', 'billboard', 'billi', 'billion', 'bin', 'bio', 'biograph', 'biographi', 'biolog', 'bird', 'birth', 'birthday', 'bishonen', 'bishop', 'bit', 'bitch', 'bite', 'bitter', 'bizarr', 'biznitch', 'black', 'blah', 'blame', 'blank', 'blanket', 'blast', 'blatant', 'bleachanhero', 'bless', 'blind', 'block', 'blog', 'blogger', 'blogspot', 'blood', 'bloodi', 'blow', 'blp', 'blue', 'blunt', 'blurb', 'bnp', 'bo', 'board', 'boat', 'bob', 'bodi', 'boe', 'bogus', 'boil', 'bold', 'bollock', 'bomb', 'bond', 'bone', 'bongwarriorcongratualt', 'boo', 'book', 'boot', 'border', 'borderlin', 'bore', 'born', 'borrow', 'bosnia', 'bosnian', 'boss', 'boston', 'bot', 'bother', 'bottl', 'bottom', 'bought', 'bound', 'boundari', 'bout', 'bow', 'bowl', 'box', 'boy', 'boyfriend', 'boymama', 'br', 'bracket', 'brain', 'branch', 'brand', 'brave', 'brazil', 'brazilian', 'brd', 'breach', 'break', 'breast', 'breath', 'breed', 'brian', 'bridg', 'brief', 'briefli', 'bright', 'brilliant', 'bring', 'britain', 'britannica', 'british', 'bro', 'broad', 'broadcast', 'broader', 'broke', 'broken', 'brook', 'brother', 'brought', 'brown', 'brows', 'browser', 'bruce', 'brush', 'brutal', 'bs', 'btw', 'buddha', 'buddhism', 'buddhist', 'buddi', 'budget', 'bug', 'build', 'built', 'bulgaria', 'bulgarian', 'bulk', 'bull', 'bullet', 'bulli', 'bullshit', 'bum', 'bunch', 'bunkstev', 'burden', 'bureaucrat', 'buri', 'burn', 'bus', 'bush', 'busi', 'bust', 'butt', 'butter', 'button', 'buttseck', 'buy', 'bye', 'byte', 'byzantin', 'c', 'ca', 'cabal', 'cabl', 'cach', 'cake', 'calcul', 'calendar', 'calgari', 'california', 'call', 'calm', 'cambridg', 'came', 'camera', 'camp', 'campaign', 'campbel', 'campus', 'canada', 'canadian', 'cancel', 'cancer', 'candid', 'cannot', 'canon', 'canvass', 'cap', 'capabl', 'capac', 'capit', 'capitalis', 'captain', 'caption', 'captur', 'car', 'carbon', 'card', 'care', 'career', 'caribbean', 'carl', 'carlo', 'carolina', 'carri', 'carrier', 'carrot', 'carter', 'cartoon', 'case', 'cash', 'cast', 'castl', 'casual', 'casualti', 'cat', 'catch', 'categor', 'categori', 'cathol', 'catholic', 'caucasian', 'caught', 'caus', 'caution', 'cave', 'cbs', 'cc', 'cd', 'ce', 'ceas', 'celebr', 'cell', 'cellpad', 'cellspac', 'celtic', 'censor', 'censorship', 'census', 'cent', 'center', 'centr', 'central', 'centuri', 'ceo', 'ceremoni', 'certain', 'certif', 'certifi', 'cf', 'cfd', 'ch', 'chain', 'chair', 'chairman', 'challeng', 'chamar', 'chamber', 'champion', 'championship', 'chan', 'chanc', 'chang', 'channel', 'chao', 'chapter', 'charact', 'character', 'characterist', 'charg', 'chariti', 'charl', 'charli', 'chart', 'charter', 'chase', 'chat', 'che', 'cheap', 'cheat', 'cheatsheet', 'check', 'checkus', 'cheer', 'chees', 'cheesei', 'chemic', 'chemistri', 'cherri', 'chess', 'chester', 'chicago', 'chicken', 'chief', 'child', 'childhood', 'childish', 'children', 'chill', 'china', 'chines', 'chip', 'chiropract', 'chocobo', 'choic', 'chola', 'choos', 'chose', 'chosen', 'chris', 'christ', 'christian', 'christma', 'christoph', 'chronicl', 'chronolog', 'chuck', 'chunk', 'church', 'churchil', 'cia', 'cinema', 'circl', 'circuit', 'circul', 'circular', 'circumcis', 'circumst', 'citat', 'cite', 'citi', 'citizen', 'citizenship', 'civil', 'civilian', 'ck', 'claim', 'clan', 'clarif', 'clarifi', 'clariti', 'clark', 'class', 'classic', 'classif', 'classifi', 'claus', 'clean', 'cleans', 'cleanup', 'clear', 'clearer', 'clerk', 'clever', 'click', 'client', 'climat', 'cline', 'clinic', 'clinton', 'clip', 'cliqu', 'clock', 'clone', 'close', 'closer', 'closest', 'closur', 'cloth', 'cloud', 'clown', 'club', 'clue', 'clueless', 'clutter', 'cma', 'cnn', 'co', 'coach', 'coal', 'coast', 'coat', 'cock', 'cocksuck', 'code', 'coffe', 'coher', 'coi', 'coin', 'coincid', 'cold', 'collabor', 'collaps', 'colleagu', 'collect', 'colleg', 'collin', 'colon', 'coloni', 'color', 'colour', 'columbia', 'column', 'com', 'combat', 'combin', 'come', 'comedi', 'comfort', 'comic', 'comma', 'command', 'commend', 'comment', 'commentari', 'commerci', 'commiss', 'commit', 'committe', 'common', 'commonwealth', 'communic', 'communism', 'communist', 'communiti', 'compani', 'compar', 'comparison', 'compat', 'compel', 'compens', 'compet', 'competit', 'competitor', 'compil', 'complain', 'complaint', 'complet', 'complex', 'compli', 'complianc', 'complic', 'compliment', 'compon', 'compos', 'composit', 'compound', 'comprehend', 'comprehens', 'compris', 'compromis', 'comput', 'con', 'conceal', 'conced', 'conceiv', 'concensus', 'concentr', 'concept', 'concern', 'concernthank', 'concert', 'concis', 'conclud', 'conclus', 'concret', 'concur', 'condemn', 'condens', 'condescend', 'condit', 'conduct', 'confeder', 'confer', 'confess', 'confid', 'confin', 'confirm', 'conflict', 'conform', 'confront', 'confus', 'congrat', 'congratul', 'congress', 'connect', 'connolley', 'connot', 'conquer', 'conscious', 'consensus', 'consent', 'consequ', 'conserv', 'consid', 'consider', 'consist', 'consol', 'conspiraci', 'constant', 'constitut', 'constru', 'construct', 'consult', 'consum', 'cont', 'contact', 'contain', 'contemporari', 'contempt', 'contend', 'content', 'contenti', 'contest', 'context', 'contextu', 'contin', 'continu', 'contra', 'contract', 'contradict', 'contradictori', 'contrari', 'contrast', 'contrib', 'contribut', 'contributor', 'control', 'controversi', 'conveni', 'convent', 'convers', 'convert', 'convey', 'convict', 'convinc', 'cook', 'cooki', 'cool', 'cooper', 'coordin', 'cop', 'copi', 'copyedit', 'copyright', 'copyvio', 'core', 'corner', 'corp', 'corpor', 'correct', 'correl', 'correspond', 'corrobor', 'corrupt', 'cos', 'cost', 'could', 'council', 'counsel', 'count', 'counter', 'counti', 'countless', 'countri', 'coup', 'coupl', 'courag', 'cours', 'court', 'courtesi', 'cousin', 'cover', 'coverag', 'cow', 'coward', 'cp', 'crack', 'craig', 'crap', 'crash', 'crazi', 'creat', 'creation', 'creationist', 'creativ', 'creator', 'creatur', 'credenti', 'credibl', 'credit', 'creek', 'crew', 'cri', 'cricket', 'crime', 'crimin', 'criminalwar', 'crisi', 'criteria', 'criterion', 'critic', 'criticis', 'critiqu', 'croat', 'croatia', 'croatian', 'crop', 'cross', 'crow', 'crowd', 'crown', 'crucial', 'crusad', 'crush', 'crystal', 'cs', 'csd', 'ct', 'cuban', 'cuisin', 'cult', 'cultur', 'cum', 'cunt', 'cuntfrank', 'cuntliz', 'cup', 'cur', 'curat', 'cure', 'curios', 'curious', 'currenc', 'current', 'curri', 'curs', 'curv', 'custom', 'cut', 'cute', 'cuz', 'cyber', 'cycl', 'cypriot', 'cyprus', 'czech', 'da', 'dab', 'dad', 'daedalus', 'daili', 'dalla', 'damag', 'damn', 'dan', 'danc', 'danger', 'daniel', 'danish', 'danni', 'dare', 'dark', 'darwin', 'dash', 'data', 'databas', 'date', 'daughter', 'dave', 'davi', 'david', 'dawn', 'day', 'db', 'dc', 'de', 'dead', 'deal', 'dealt', 'dean', 'dear', 'death', 'debat', 'debt', 'debunk', 'debut', 'dec', 'decad', 'deceas', 'decemb', 'decent', 'decept', 'decid', 'decis', 'declar', 'declin', 'decor', 'decreas', 'dedic', 'deem', 'deep', 'deepli', 'defam', 'defamatori', 'default', 'defeat', 'defenc', 'defend', 'defens', 'defin', 'definit', 'degrad', 'degre', 'del', 'delay', 'deleg', 'delet', 'deletionist', 'delhi', 'deliber', 'delight', 'deliv', 'deliveri', 'demand', 'democraci', 'democrat', 'demograph', 'demon', 'demonstr', 'deneid', 'deni', 'denial', 'denni', 'denomin', 'depart', 'depend', 'depict', 'deploy', 'depress', 'depth', 'der', 'deriv', 'derogatori', 'des', 'descend', 'descent', 'describ', 'descript', 'desert', 'deserv', 'design', 'desir', 'desist', 'desk', 'desper', 'despit', 'destin', 'destroy', 'destruct', 'detail', 'detect', 'determin', 'detriment', 'detroit', 'develop', 'devic', 'devil', 'devot', 'di', 'diacrit', 'diagram', 'dialect', 'dialogu', 'dick', 'dickhead', 'dictat', 'dictatorship', 'dictionari', 'didnt', 'die', 'diego', 'diet', 'diff', 'differ', 'differenti', 'difficult', 'difficulti', 'dig', 'digit', 'dilig', 'dimens', 'diminish', 'dinner', 'diplomat', 'dipshit', 'direct', 'director', 'directori', 'dirti', 'disabl', 'disagr', 'disagre', 'disambig', 'disambigu', 'disappear', 'disappoint', 'disast', 'disc', 'disciplin', 'disclaim', 'disclos', 'disco', 'discographi', 'discourag', 'discours', 'discov', 'discoveri', 'discredit', 'discrep', 'discret', 'discrimin', 'discuss', 'diseas', 'disgrac', 'disguis', 'disgust', 'dish', 'dishonest', 'dislik', 'dismiss', 'disney', 'disord', 'display', 'disprov', 'disput', 'disregard', 'disrespect', 'disrupt', 'dissent', 'distanc', 'distinct', 'distinguish', 'distort', 'distract', 'distribut', 'district', 'disturb', 'dive', 'divers', 'divid', 'divin', 'divis', 'divorc', 'dj', 'dna', 'doc', 'doctor', 'doctrin', 'document', 'documentari', 'dodo', 'doe', 'doesnt', 'dog', 'dollar', 'domain', 'domest', 'domin', 'dominican', 'donat', 'done', 'donkey', 'dont', 'doom', 'door', 'dose', 'dot', 'doubl', 'doubt', 'douch', 'douchebag', 'doug', 'download', 'dozen', 'dq', 'dr', 'draft', 'drag', 'dragon', 'drama', 'dramat', 'draw', 'drawn', 'dread', 'dream', 'dress', 'drew', 'dri', 'drink', 'drive', 'driven', 'driver', 'drmi', 'drop', 'dropdown', 'drug', 'drum', 'drunk', 'drv', 'du', 'dual', 'dub', 'dubious', 'duck', 'dude', 'due', 'duke', 'dumb', 'dumbass', 'dump', 'dunno', 'duplic', 'dure', 'dust', 'dutch', 'duti', 'dvd', 'dyk', 'dylan', 'dynam', 'dynasti', 'e', 'eager', 'eagl', 'ear', 'earl', 'earli', 'earlier', 'earliest', 'earn', 'earth', 'easi', 'easier', 'easili', 'east', 'eastern', 'eat', 'ebay', 'ec', 'echo', 'econom', 'economi', 'economist', 'ed', 'eddi', 'edg', 'edgar', 'edi', 'edit', 'editor', 'editori', 'edu', 'educ', 'edward', 'ee', 'effect', 'effici', 'effort', 'eg', 'egg', 'ego', 'egypt', 'egyptian', 'eh', 'eight', 'einstein', 'either', 'el', 'elabor', 'elder', 'elect', 'elector', 'electr', 'electron', 'element', 'elementari', 'elev', 'eleven', 'elig', 'elimin', 'elit', 'elizabeth', 'elonka', 'els', 'elsewher', 'em', 'email', 'embarrass', 'emerg', 'emo', 'emot', 'emperor', 'emphas', 'emphasi', 'empir', 'employ', 'employe', 'empti', 'en', 'enabl', 'encompass', 'encount', 'encourag', 'encyclopaed', 'encyclopaedia', 'encycloped', 'encyclopedia', 'end', 'endless', 'endors', 'endur', 'enemi', 'energi', 'enforc', 'engag', 'engin', 'england', 'english', 'enhanc', 'enigmaman', 'enjoy', 'enlighten', 'enorm', 'enough', 'ensur', 'enter', 'enterpris', 'entertain', 'enthusiast', 'entir', 'entiti', 'entitl', 'entri', 'environ', 'environment', 'ep', 'epic', 'episod', 'equal', 'equat', 'equip', 'equival', 'er', 'era', 'eras', 'eric', 'erron', 'error', 'es', 'escal', 'escap', 'esp', 'especi', 'essay', 'essenc', 'essenti', 'est', 'establish', 'estat', 'estim', 'et', 'etc', 'etern', 'ethic', 'ethnic', 'etiquett', 'etymolog', 'eu', 'europ', 'european', 'evad', 'evalu', 'evan', 'evas', 'even', 'event', 'eventu', 'ever', 'everi', 'everybodi', 'everyday', 'everyon', 'everyth', 'everytim', 'everywher', 'evid', 'evil', 'evolut', 'evolutionari', 'evolv', 'ex', 'exact', 'exagger', 'examin', 'exampl', 'exceed', 'excel', 'except', 'excerpt', 'excess', 'exchang', 'excit', 'exclud', 'exclus', 'excus', 'execut', 'exempt', 'exercis', 'exhaust', 'exhibit', 'exil', 'exist', 'exit', 'expand', 'expans', 'expect', 'expel', 'expens', 'experi', 'experienc', 'experiment', 'expert', 'expertis', 'expir', 'explain', 'explan', 'explicit', 'exploit', 'explor', 'explos', 'expos', 'exposur', 'express', 'extend', 'extens', 'extent', 'extern', 'extinct', 'extra', 'extract', 'extrem', 'extremist', 'eye', 'f', 'fa', 'fabric', 'fac', 'face', 'facebook', 'facil', 'facilit', 'fact', 'facto', 'factor', 'factori', 'factual', 'faculti', 'fag', 'faggot', 'fail', 'failep', 'failur', 'fair', 'faith', 'fake', 'falcon', 'fall', 'fallaci', 'fals', 'falsehood', 'falsifi', 'fame', 'famili', 'familiar', 'famous', 'fan', 'fanat', 'fanboy', 'fanci', 'fantasi', 'fantast', 'faq', 'far', 'farm', 'fart', 'fascin', 'fascism', 'fascist', 'fashion', 'fast', 'faster', 'fat', 'fatal', 'fate', 'father', 'fatuorum', 'fault', 'favor', 'favorit', 'favour', 'favourit', 'fbi', 'fc', 'fear', 'featur', 'feb', 'februari', 'fed', 'feder', 'fee', 'feed', 'feedback', 'feel', 'feet', 'fell', 'fellow', 'felt', 'femal', 'femin', 'feminist', 'festiv', 'fewer', 'fffa', 'ffffff', 'fggt', 'fi', 'fiction', 'field', 'fifth', 'fight', 'fighter', 'figur', 'file', 'fill', 'film', 'filter', 'final', 'financ', 'financi', 'find', 'fine', 'finger', 'finish', 'finland', 'finnish', 'fire', 'firefox', 'firm', 'first', 'fish', 'fit', 'five', 'fix', 'fl', 'flag', 'flame', 'flash', 'flat', 'flaw', 'fleet', 'flesh', 'fli', 'flight', 'flip', 'flood', 'floor', 'florida', 'flow', 'flower', 'focus', 'fold', 'folk', 'follow', 'font', 'food', 'fool', 'foolish', 'foot', 'footbal', 'footnot', 'forbid', 'forbidden', 'forc', 'ford', 'foreign', 'forest', 'forev', 'forget', 'forgiv', 'forgot', 'forgotten', 'fork', 'form', 'formal', 'format', 'former', 'formul', 'formula', 'fort', 'forth', 'fortun', 'forum', 'forward', 'fossil', 'fought', 'foul', 'found', 'foundat', 'founder', 'four', 'fourth', 'fox', 'fr', 'fraction', 'fragment', 'frame', 'franc', 'franchis', 'franci', 'francisco', 'frank', 'franklin', 'fratern', 'fraud', 'freak', 'fred', 'free', 'freedom', 'freeli', 'freezer', 'french', 'frequenc', 'frequent', 'fresh', 'fri', 'friday', 'friend', 'friggen', 'fring', 'front', 'fruit', 'frustrat', 'ft', 'fu', 'fuck', 'fucker', 'fuckin', 'fuckingabf', 'fucksex', 'fuel', 'fulfil', 'full', 'fulli', 'fun', 'function', 'fund', 'fundament', 'fundamentalist', 'funni', 'furthermor', 'fusion', 'futil', 'futur', 'fyi', 'fyrom', 'g', 'ga', 'gain', 'gale', 'galleri', 'gamaliel', 'game', 'gan', 'gang', 'gap', 'garbag', 'garden', 'gari', 'gas', 'gate', 'gather', 'gave', 'gay', 'gayfag', 'gaza', 'gdp', 'gear', 'gee', 'geek', 'gender', 'gene', 'general', 'generat', 'generic', 'genesi', 'genet', 'genit', 'genius', 'genocid', 'genr', 'genuin', 'genus', 'geo', 'geograph', 'geographi', 'geolog', 'georg', 'georgia', 'georgian', 'german', 'germani', 'get', 'gfdl', 'gg', 'ghost', 'giant', 'gibson', 'gif', 'gift', 'girl', 'girlfriend', 'give', 'given', 'glad', 'glanc', 'glass', 'global', 'gmail', 'gmt', 'gnu', 'go', 'goal', 'goat', 'god', 'goddamn', 'goe', 'gold', 'golden', 'gon', 'gone', 'good', 'goodby', 'googl', 'gospel', 'gossip', 'got', 'goth', 'gothic', 'gotten', 'gov', 'govern', 'governor', 'gr', 'grab', 'grace', 'grade', 'gradual', 'graduat', 'graham', 'grammar', 'grammat', 'grand', 'grandfath', 'grant', 'graph', 'graphic', 'grasp', 'grate', 'grave', 'gravit', 'graviti', 'gray', 'great', 'greater', 'greatest', 'greec', 'greek', 'green', 'greet', 'grew', 'grey', 'griffin', 'gross', 'ground', 'group', 'grow', 'grown', 'growth', 'grudg', 'guarante', 'guard', 'guardian', 'guess', 'guest', 'guid', 'guidanc', 'guidelin', 'guild', 'guilt', 'guilti', 'guitar', 'gulf', 'gun', 'guru', 'gut', 'guy', 'gwen', 'gwernol', 'h', 'ha', 'haahhahahah', 'habit', 'hack', 'hacker', 'hadith', 'haha', 'hahaha', 'hail', 'hair', 'hairi', 'half', 'hall', 'hama', 'hammer', 'han', 'hand', 'handl', 'hang', 'hanib', 'happen', 'happi', 'happili', 'harass', 'hard', 'hardcor', 'harder', 'harm', 'harrass', 'harri', 'harsh', 'harvard', 'hat', 'hate', 'hater', 'hatr', 'havent', 'hawaii', 'hawk', 'hawkinghttp', 'hay', 'hazel', 'hd', 'head', 'header', 'headlin', 'health', 'healthi', 'hear', 'heard', 'heart', 'heat', 'heaven', 'heavi', 'heavili', 'hebrew', 'heck', 'heh', 'height', 'heil', 'held', 'helen', 'hell', 'hello', 'help', 'helpm', 'henc', 'henri', 'herald', 'herebi', 'heritag', 'hero', 'hes', 'hesit', 'hey', 'hezbollah', 'hi', 'hidden', 'hide', 'high', 'higher', 'highest', 'highlight', 'highway', 'hijack', 'hilari', 'hill', 'hindi', 'hindu', 'hinduism', 'hindus', 'hint', 'hip', 'hire', 'hispan', 'hist', 'histor', 'histori', 'historian', 'hit', 'hitler', 'hiv', 'hl', 'hm', 'hmm', 'hmmm', 'ho', 'hoax', 'hobbi', 'hockey', 'hold', 'holder', 'hole', 'holi', 'holiday', 'hollywood', 'holocaust', 'home', 'homeopathi', 'homepag', 'homework', 'hominem', 'homo', 'homophob', 'homosexu', 'honest', 'honesti', 'hong', 'honor', 'honour', 'hoo', 'hood', 'hook', 'hop', 'hope', 'horribl', 'horror', 'hors', 'hospit', 'host', 'hostil', 'hot', 'hotel', 'hound', 'hour', 'hous', 'howard', 'howev', 'hp', 'htm', 'html', 'http', 'https', 'hu', 'hub', 'huge', 'hugh', 'huh', 'human', 'humbl', 'humor', 'humour', 'hundr', 'hungari', 'hungarian', 'hunt', 'hunter', 'hurrican', 'hurt', 'husband', 'hyphen', 'hypocrisi', 'hypocrit', 'hypothesi', 'hypothet', 'ian', 'ibn', 'ice', 'icon', 'id', 'idea', 'ideal', 'ident', 'identif', 'identifi', 'ideolog', 'idiot', 'ie', 'ignor', 'ii', 'iii', 'il', 'ill', 'illeg', 'illog', 'illustr', 'illyrian', 'im', 'imag', 'imagin', 'imaginari', 'imdb', 'imho', 'immatur', 'immedi', 'immigr', 'immun', 'imo', 'impact', 'imparti', 'imperi', 'imperson', 'implement', 'impli', 'implic', 'import', 'impos', 'imposs', 'impress', 'improp', 'improv', 'inaccur', 'inaccuraci', 'inact', 'inadequ', 'inadvert', 'inappropri', 'inc', 'incap', 'inch', 'incid', 'incident', 'incivil', 'inclin', 'includ', 'inclus', 'incoher', 'incom', 'incompet', 'incomplet', 'inconsist', 'inconveni', 'incorpor', 'incorrect', 'increas', 'incred', 'inde', 'indef', 'indefinit', 'indent', 'independ', 'index', 'indi', 'india', 'indian', 'indic', 'indigen', 'indirect', 'individu', 'indo', 'induc', 'industri', 'inevit', 'infant', 'infect', 'infer', 'inferior', 'infinit', 'inflammatori', 'inflat', 'influenc', 'influenti', 'info', 'infobox', 'inform', 'infring', 'ing', 'inhabit', 'inher', 'inherit', 'initi', 'injuri', 'injustic', 'inlin', 'inner', 'innoc', 'innov', 'input', 'inquiri', 'insan', 'insert', 'insid', 'insight', 'insignificannot', 'insinu', 'insist', 'inspir', 'instal', 'instanc', 'instant', 'instead', 'institut', 'instruct', 'instrument', 'insuffici', 'insult', 'insur', 'intact', 'integr', 'intellectu', 'intellig', 'intend', 'intens', 'intent', 'inter', 'interact', 'interest', 'interfac', 'interfer', 'intern', 'internet', 'interpret', 'interv', 'interven', 'intervent', 'interview', 'interwiki', 'intimid', 'intro', 'introduc', 'introduct', 'introductori', 'intuit', 'invad', 'invalid', 'invas', 'invent', 'invest', 'investig', 'invit', 'invok', 'involv', 'ip', 'ipa', 'iq', 'ir', 'ira', 'iran', 'iranian', 'iraq', 'iraqi', 'irc', 'ireland', 'irish', 'iron', 'ironi', 'irrat', 'irrelev', 'irrit', 'isbn', 'isi', 'isl', 'islam', 'islamist', 'island', 'isnt', 'isol', 'isp', 'isra', 'israel', 'issu', 'ital', 'itali', 'italian', 'item', 'itsuck', 'iv', 'ive', 'j', 'jack', 'jackass', 'jackson', 'jail', 'jame', 'jan', 'jane', 'januari', 'japan', 'japanes', 'jason', 'jay', 'jayjg', 'jazz', 'jealous', 'jean', 'jeff', 'jefferson', 'jehovah', 'jeremi', 'jerk', 'jersey', 'jerusalem', 'jesus', 'jet', 'jew', 'jewish', 'jihad', 'jim', 'jimbo', 'jimmi', 'job', 'joe', 'john', 'johnni', 'johnson', 'join', 'joint', 'joke', 'jon', 'jonathan', 'jone', 'jordan', 'joseph', 'josh', 'joshua', 'journal', 'journalist', 'joy', 'jpg', 'jr', 'judaism', 'judg', 'judgement', 'judgment', 'judici', 'juic', 'juici', 'jul', 'juli', 'jump', 'jun', 'june', 'junior', 'junk', 'juri', 'jurisdict', 'justic', 'justif', 'justifi', 'justin', 'k', 'kannada', 'kansa', 'karl', 'kashmir', 'kb', 'keen', 'keep', 'keith', 'kelli', 'ken', 'kennedi', 'kept', 'kerri', 'kevin', 'key', 'keyboard', 'khan', 'kick', 'kid', 'kill', 'killer', 'kim', 'kind', 'kinda', 'king', 'kingdom', 'kiss', 'kitten', 'km', 'knee', 'knew', 'knight', 'knob', 'knock', 'know', 'knowledg', 'known', 'knox', 'kong', 'korea', 'korean', 'kosovo', 'kshatriya', 'kurd', 'kurdish', 'kurt', 'kyiv', 'kyle', 'l', 'la', 'lab', 'label', 'labor', 'labour', 'lack', 'laden', 'ladi', 'laid', 'lake', 'lame', 'land', 'lane', 'languag', 'lanka', 'lao', 'larg', 'larger', 'largest', 'larri', 'last', 'late', 'later', 'latest', 'latin', 'latinus', 'latter', 'laugh', 'laughabl', 'launch', 'law', 'lawdi', 'lawrenc', 'lawsuit', 'lawyer', 'lay', 'layer', 'layout', 'lazi', 'lds', 'le', 'lead', 'leader', 'leadership', 'leagu', 'leak', 'lean', 'learn', 'least', 'leav', 'lebanes', 'lebanon', 'lectur', 'led', 'lede', 'lee', 'left', 'leftist', 'leg', 'legaci', 'legal', 'legend', 'legisl', 'legit', 'legitim', 'lend', 'length', 'lengthi', 'les', 'lesbian', 'less', 'lesser', 'lesson', 'let', 'letter', 'level', 'lewi', 'lgbt', 'li', 'liar', 'libel', 'liber', 'libertarian', 'liberti', 'librari', 'licenc', 'licens', 'lick', 'licker', 'lie', 'life', 'lifestyl', 'lift', 'light', 'lightgrey', 'like', 'likewis', 'lil', 'limit', 'lincoln', 'line', 'lineag', 'linear', 'linguist', 'link', 'linux', 'lion', 'liquid', 'list', 'lista', 'listen', 'liter', 'literari', 'literatur', 'lithuanian', 'litig', 'littl', 'live', 'liverpool', 'lk', 'lmao', 'load', 'lobbi', 'local', 'locat', 'lock', 'lodg', 'log', 'logic', 'login', 'logo', 'lol', 'lolooolbootstoot', 'london', 'lone', 'long', 'longer', 'longest', 'look', 'loop', 'loos', 'lord', 'los', 'lose', 'loser', 'loss', 'lost', 'lot', 'loud', 'loui', 'love', 'lover', 'low', 'lower', 'lowest', 'ltd', 'luck', 'lucki', 'luke', 'lyric', 'mac', 'macedonia', 'macedonian', 'machin', 'mad', 'made', 'mafia', 'magazin', 'magic', 'magnet', 'mail', 'main', 'mainpagebg', 'mainspac', 'mainstream', 'maintain', 'mainten', 'major', 'make', 'maker', 'male', 'malici', 'malik', 'mall', 'malleus', 'mama', 'man', 'manag', 'manchest', 'manga', 'mangina', 'mani', 'manifest', 'manipul', 'mankind', 'mann', 'manner', 'manual', 'manufactur', 'map', 'mar', 'marc', 'march', 'marco', 'marcolfuck', 'margin', 'mari', 'maria', 'marin', 'mario', 'mark', 'marker', 'market', 'marri', 'marriag', 'marshal', 'mart', 'martial', 'martin', 'marvel', 'marx', 'marxist', 'mask', 'mason', 'mass', 'massacr', 'massiv', 'master', 'masturb', 'match', 'mate', 'materi', 'math', 'mathemat', 'mathematician', 'matrix', 'matt', 'matter', 'matthew', 'matur', 'max', 'maximum', 'may', 'mayb', 'mayor', 'mccain', 'md', 'mean', 'meaning', 'meaningless', 'meant', 'meantim', 'meanwhil', 'measur', 'meat', 'mechan', 'med', 'medal', 'media', 'mediat', 'mediawiki', 'medic', 'medicin', 'mediev', 'medium', 'meet', 'meetup', 'mel', 'melbourn', 'member', 'membership', 'memori', 'men', 'mental', 'mention', 'mentor', 'merci', 'mercuri', 'mere', 'merg', 'merger', 'merit', 'merri', 'mess', 'messag', 'met', 'meta', 'metal', 'meter', 'method', 'methodolog', 'metric', 'metro', 'metropolitan', 'mexican', 'mexico', 'mfd', 'mg', 'mi', 'michael', 'michigan', 'microsoft', 'mid', 'middl', 'might', 'migrat', 'mike', 'mild', 'mile', 'milit', 'militari', 'mill', 'miller', 'million', 'milton', 'mind', 'mine', 'mini', 'minim', 'minimum', 'minist', 'ministri', 'minnesota', 'minor', 'minut', 'mirror', 'mis', 'miscellani', 'misconcept', 'miser', 'misguid', 'misinform', 'misinterpret', 'mislead', 'misread', 'misrepres', 'misrepresent', 'miss', 'missil', 'mission', 'misspel', 'mistak', 'mistaken', 'misunderstand', 'misunderstood', 'misus', 'mitt', 'mix', 'mm', 'mo', 'mob', 'mobil', 'mock', 'mod', 'mode', 'model', 'moder', 'modern', 'modif', 'modifi', 'modul', 'moham', 'moldovan', 'molest', 'mom', 'moment', 'mon', 'monarch', 'monday', 'money', 'mongo', 'mongol', 'monitor', 'monkey', 'monster', 'montenegro', 'month', 'monument', 'mood', 'moon', 'moor', 'moot', 'moral', 'moreov', 'mormon', 'morn', 'moron', 'mortal', 'mos', 'mosqu', 'mother', 'motherfuck', 'mothjer', 'motion', 'motiv', 'motor', 'motto', 'mount', 'mountain', 'mous', 'mouth', 'move', 'movement', 'movi', 'mp', 'mr', 'mrs', 'ms', 'msg', 'mtv', 'muahahahahahahahahahahahahahahahahahaha', 'much', 'muhammad', 'multi', 'multipl', 'mum', 'municip', 'murder', 'museum', 'music', 'musician', 'muslim', 'must', 'mutual', 'myspac', 'mysteri', 'myth', 'mytholog', 'n', 'na', 'nail', 'nair', 'nake', 'name', 'namespac', 'nanci', 'narrat', 'narrow', 'nasa', 'nasti', 'nation', 'nationalist', 'nativ', 'nato', 'natur', 'naughti', 'naval', 'navi', 'navig', 'nazi', 'nazism', 'nba', 'nd', 'ne', 'near', 'neat', 'necessari', 'necessarili', 'neck', 'need', 'needless', 'negat', 'neglect', 'negoti', 'negro', 'neighbor', 'neighborhood', 'neiln', 'neither', 'nelson', 'neo', 'neolog', 'nerd', 'nerv', 'net', 'netherland', 'network', 'neutral', 'never', 'nevertheless', 'new', 'newbi', 'newcom', 'newer', 'newli', 'news', 'newslett', 'newspap', 'newton', 'next', 'nfl', 'ng', 'nhrhs', 'ni', 'nice', 'nichola', 'nick', 'nicknam', 'nigga', 'nigger', 'night', 'nightmar', 'nikko', 'nine', 'nintendo', 'nippl', 'nixon', 'njgw', 'nl', 'nobel', 'nobl', 'nobodi', 'noe', 'nois', 'nom', 'nomin', 'non', 'none', 'nonetheless', 'nonsens', 'noob', 'noon', 'nope', 'norm', 'normal', 'norman', 'norri', 'north', 'northern', 'norway', 'norwegian', 'nose', 'notabl', 'notat', 'note', 'noteworthi', 'noth', 'notic', 'noticeboard', 'notif', 'notifi', 'notion', 'notori', 'notrhbysouthbanof', 'noun', 'nov', 'novel', 'novemb', 'nowaday', 'nowher', 'npa', 'npov', 'nt', 'nu', 'nuclear', 'number', 'numer', 'nurs', 'nut', 'ny', 'nyc', 'nz', 'obama', 'object', 'oblig', 'obscur', 'observ', 'obsess', 'obtain', 'obvious', 'occas', 'occasion', 'occup', 'occupi', 'occur', 'ocean', 'oct', 'octob', 'odd', 'offenc', 'offend', 'offens', 'offer', 'offfuck', 'offic', 'offici', 'offlin', 'often', 'oh', 'ohio', 'oi', 'oil', 'ok', 'okay', 'old', 'older', 'oldest', 'oldid', 'oliv', 'olymp', 'omega', 'omg', 'omit', 'onc', 'one', 'ongo', 'onli', 'onlin', 'ontario', 'onto', 'oop', 'op', 'open', 'oper', 'opera', 'opinion', 'oppon', 'opportun', 'oppos', 'opposit', 'oppress', 'opt', 'optic', 'option', 'oral', 'orang', 'orbit', 'order', 'ordin', 'ordinari', 'oregon', 'org', 'organ', 'organis', 'orient', 'origin', 'orphan', 'orthodox', 'os', 'oswald', 'ot', 'otherwis', 'otr', 'otter', 'ottoman', 'ought', 'ourselv', 'outcom', 'outdat', 'outing', 'outlet', 'outlin', 'output', 'outrag', 'outright', 'outsid', 'outstand', 'overal', 'overlap', 'overlook', 'oversight', 'overturn', 'overview', 'overwhelm', 'owe', 'owner', 'ownership', 'oxford', 'oxymoron', 'p', 'pa', 'pacif', 'pack', 'packag', 'pad', 'pagan', 'page', 'pagedelet', 'paid', 'pain', 'paint', 'pair', 'pakistan', 'pakistani', 'pal', 'palestin', 'palestinian', 'palin', 'pan', 'panel', 'pant', 'paper', 'par', 'para', 'paradox', 'paragraph', 'parallel', 'paramet', 'paranoid', 'paraphras', 'pardon', 'parent', 'pari', 'park', 'parliament', 'parodi', 'part', 'parti', 'partial', 'particip', 'particl', 'particular', 'partisan', 'partner', 'pashtun', 'pass', 'passag', 'passeng', 'passion', 'passiv', 'passport', 'password', 'past', 'pat', 'patent', 'path', 'pathet', 'patienc', 'patient', 'patrick', 'patriot', 'patrol', 'patron', 'pattern', 'paul', 'pay', 'payment', 'pbs', 'pc', 'pd', 'pdf', 'peac', 'peak', 'pedia', 'pedophil', 'peer', 'pejor', 'pen', 'pend', 'peni', 'penni', 'pennni', 'pennsylvania', 'pensnsnniensnsn', 'peopl', 'per', 'perceiv', 'percent', 'percentag', 'percept', 'perfect', 'perform', 'perhap', 'period', 'perman', 'permiss', 'permit', 'perpetr', 'perpetu', 'persecut', 'persian', 'persist', 'person', 'personnel', 'perspect', 'persuad', 'pertain', 'pertin', 'pet', 'pete', 'peter', 'petit', 'petti', 'pg', 'ph', 'phase', 'phd', 'phenomenon', 'phil', 'philadelphia', 'philip', 'philippin', 'philippineslong', 'phillip', 'philosoph', 'philosophi', 'phone', 'photo', 'photograph', 'photographi', 'photoshop', 'php', 'phrase', 'phuq', 'physic', 'physician', 'physicist', 'piano', 'pic', 'pick', 'pictur', 'pie', 'piec', 'pig', 'pile', 'pillar', 'pilot', 'ping', 'pink', 'pipe', 'pirat', 'piss', 'pit', 'pitch', 'piti', 'pl', 'place', 'placement', 'plagiar', 'plain', 'plan', 'plane', 'planet', 'plant', 'plate', 'platform', 'plausibl', 'play', 'player', 'pleas', 'pleasur', 'pledg', 'plenti', 'plot', 'pls', 'plural', 'plus', 'plz', 'pm', 'pnei', 'png', 'po', 'pocket', 'poem', 'poet', 'poetri', 'point', 'pointer', 'pointless', 'poison', 'poland', 'pole', 'polic', 'polici', 'polish', 'polit', 'politician', 'poll', 'pollut', 'pompous', 'pool', 'poor', 'pop', 'pope', 'popul', 'popular', 'porn', 'pornographi', 'port', 'portal', 'portion', 'portrait', 'portray', 'portug', 'portugues', 'pose', 'posit', 'possess', 'possibl', 'post', 'poster', 'pot', 'potenti', 'potter', 'pound', 'pour', 'pov', 'poverti', 'power', 'pp', 'pr', 'practic', 'practition', 'pradesh', 'prais', 'pray', 'prayer', 'pre', 'preach', 'preced', 'precious', 'precis', 'predat', 'predict', 'predomin', 'prefer', 'prejudic', 'prematur', 'premier', 'premis', 'prepar', 'presenc', 'present', 'preserv', 'presid', 'presidenti', 'press', 'pressur', 'presum', 'pretend', 'pretti', 'prev', 'prevail', 'preval', 'prevent', 'preview', 'previous', 'price', 'prick', 'pride', 'priest', 'primari', 'primarili', 'prime', 'princ', 'princess', 'princip', 'principl', 'print', 'prior', 'prioriti', 'prison', 'privaci', 'privat', 'privileg', 'prize', 'pro', 'probabl', 'problem', 'problemat', 'procedur', 'proceed', 'process', 'proclaim', 'prod', 'produc', 'product', 'prof', 'profan', 'profess', 'profession', 'professor', 'profil', 'profit', 'profound', 'program', 'programm', 'progress', 'prohibit', 'project', 'promin', 'promis', 'promo', 'promot', 'prompt', 'pronounc', 'pronunci', 'proof', 'propaganda', 'propagandist', 'proper', 'properti', 'propheci', 'prophet', 'propon', 'proport', 'propos', 'proposit', 'prose', 'prosecut', 'prostitut', 'protect', 'protest', 'proto', 'protocol', 'proud', 'prove', 'proven', 'provid', 'provinc', 'provis', 'provoc', 'provok', 'proxi', 'ps', 'pseudo', 'pseudosci', 'psycholog', 'pt', 'public', 'publish', 'pubm', 'puerto', 'pull', 'pump', 'punch', 'punctuat', 'punish', 'punjab', 'punk', 'puppet', 'puppetri', 'puppi', 'purchas', 'pure', 'purg', 'purpl', 'purport', 'purpos', 'pursu', 'push', 'pusher', 'pussi', 'put', 'puzzl', 'px', 'q', 'qaeda', 'qualif', 'qualifi', 'qualiti', 'quantiti', 'quantum', 'quarter', 'quebec', 'queen', 'queer', 'queri', 'question', 'quick', 'quiet', 'quit', 'quo', 'quot', 'quotat', 'quran', 'r', 'rabbi', 'race', 'racial', 'racism', 'racist', 'radiat', 'radic', 'radio', 'rage', 'raid', 'rail', 'railroad', 'railway', 'rain', 'rais', 'raja', 'rajput', 'ralli', 'ram', 'rambl', 'ramdasia', 'ramon', 'ran', 'randi', 'random', 'rang', 'ranger', 'rank', 'rant', 'rap', 'rape', 'rapid', 'rapper', 'rare', 'rat', 'rate', 'rather', 'ratio', 'ration', 'rational', 'raul', 'raw', 'ray', 'raymond', 'rd', 'reach', 'react', 'reaction', 'read', 'readabl', 'reader', 'readi', 'readili', 'real', 'realis', 'realist', 'realiti', 'realiz', 'realli', 'realm', 'reason', 'rebel', 'rebutt', 'recal', 'receiv', 'recent', 'recept', 'reciev', 'recogn', 'recognis', 'recognit', 'recommend', 'reconsid', 'record', 'recov', 'recreat', 'recruit', 'recur', 'red', 'redact', 'redirect', 'redlink', 'reduc', 'reduct', 'redund', 'ref', 'refactor', 'refer', 'referenc', 'refin', 'reflect', 'reform', 'refrain', 'refresh', 'refuge', 'refus', 'refut', 'regard', 'regardless', 'regim', 'region', 'regist', 'registr', 'registri', 'regret', 'regul', 'regular', 'reign', 'reinforc', 'reinsert', 'reinstat', 'reiter', 'reject', 'relat', 'relationship', 'relax', 'releas', 'relev', 'reli', 'reliabl', 'relief', 'religi', 'religion', 'reluct', 'remain', 'remark', 'remedi', 'rememb', 'remind', 'remot', 'remov', 'renam', 'render', 'renew', 'renown', 'repair', 'repeat', 'repetit', 'rephras', 'replac', 'repli', 'report', 'repost', 'repres', 'represent', 'reproduc', 'republ', 'republican', 'reput', 'request', 'requir', 'rescu', 'research', 'resembl', 'resent', 'reserv', 'resid', 'resign', 'resist', 'resolut', 'resolv', 'reson', 'resort', 'resourc', 'respect', 'respond', 'respons', 'rest', 'restat', 'restaur', 'restor', 'restrict', 'result', 'resum', 'retain', 'retali', 'retard', 'retir', 'retract', 'retriev', 'return', 'rev', 'reveal', 'revel', 'reveng', 'revenu', 'revers', 'revert', 'review', 'revis', 'revisit', 'reviv', 'revok', 'revolut', 'revolutionari', 'reward', 'reword', 'rework', 'rewrit', 'rewritten', 'rex', 'rexcurri', 'rfa', 'rfc', 'rhetor', 'rice', 'rich', 'richard', 'rick', 'rid', 'ride', 'ridicul', 'rifl', 'right', 'righteous', 'ring', 'riot', 'rip', 'rise', 'risk', 'river', 'rm', 'road', 'rob', 'robert', 'robot', 'roc', 'rock', 'rocket', 'rod', 'roger', 'role', 'roll', 'rollback', 'roman', 'romanc', 'romania', 'romanian', 'rome', 'romney', 'ron', 'ronald', 'room', 'root', 'rose', 'ross', 'roster', 'rot', 'rotat', 'rough', 'round', 'rout', 'routin', 'row', 'roy', 'royal', 'rr', 'rs', 'rt', 'ru', 'rubbish', 'rude', 'rugbi', 'ruin', 'rule', 'ruler', 'rumor', 'rumour', 'run', 'rush', 'russel', 'russia', 'russian', 'rv', 'ryan', 'ryulong', 'sa', 'sacr', 'sacrific', 'sad', 'saddam', 'safe', 'safeti', 'saget', 'said', 'sail', 'saint', 'sake', 'sale', 'salt', 'salut', 'sam', 'sampl', 'samuel', 'san', 'sanchez', 'sanction', 'sand', 'sandbox', 'sanskrit', 'santa', 'sarah', 'sarcasm', 'sarcast', 'sat', 'satan', 'satellit', 'satisfi', 'saturday', 'saudi', 'savag', 'save', 'saw', 'saxon', 'say', 'sbs', 'scale', 'scan', 'scandal', 'scare', 'scenario', 'scene', 'schedul', 'scheme', 'scholar', 'scholarship', 'school', 'scienc', 'scientif', 'scientist', 'scientolog', 'scope', 'score', 'scot', 'scotland', 'scott', 'scottish', 'scout', 'scratch', 'scream', 'screen', 'screenshot', 'screw', 'script', 'scriptur', 'scroll', 'scrutini', 'scum', 'se', 'sea', 'seal', 'sean', 'search', 'season', 'seat', 'seattl', 'second', 'secondari', 'secret', 'secretari', 'sect', 'section', 'sector', 'secular', 'secur', 'see', 'seed', 'seek', 'seem', 'seen', 'segment', 'select', 'self', 'sell', 'semant', 'semen', 'semi', 'semit', 'senat', 'send', 'senior', 'sens', 'sensibl', 'sensit', 'sent', 'sentenc', 'sentiment', 'sep', 'separ', 'seper', 'sept', 'septemb', 'sequenc', 'serb', 'serbia', 'serbian', 'serer', 'seri', 'serial', 'serious', 'serv', 'server', 'servic', 'session', 'set', 'settl', 'settlement', 'seven', 'sever', 'sex', 'sexi', 'sexsex', 'sexual', 'sh', 'shadow', 'shah', 'shakespear', 'shall', 'shame', 'shannon', 'shape', 'share', 'shed', 'sheet', 'shell', 'shia', 'shield', 'shift', 'shine', 'ship', 'shirt', 'shit', 'shithol', 'shitti', 'shock', 'shoe', 'shoot', 'shop', 'short', 'shorten', 'shorter', 'shot', 'shoulder', 'shout', 'shove', 'show', 'shown', 'shut', 'si', 'sic', 'sick', 'side', 'sig', 'sigh', 'sight', 'sign', 'signal', 'signatur', 'signifi', 'signific', 'significannot', 'signpost', 'sikh', 'silenc', 'silent', 'silli', 'silver', 'similar', 'simon', 'simpl', 'simpler', 'simpli', 'simplifi', 'simpson', 'simultan', 'sin', 'sinc', 'sincer', 'sing', 'singapor', 'singer', 'singh', 'singl', 'singular', 'sinn', 'sir', 'sister', 'sit', 'site', 'situat', 'sitush', 'six', 'size', 'sk', 'skeptic', 'sketch', 'skill', 'skin', 'skip', 'sky', 'slander', 'slang', 'slant', 'slap', 'slash', 'slav', 'slave', 'slaveri', 'slavic', 'sleep', 'slight', 'slightest', 'slim', 'slimvirgin', 'slip', 'sloppi', 'slow', 'slowli', 'slur', 'slut', 'smackdown', 'small', 'smaller', 'smart', 'smash', 'smear', 'smell', 'smile', 'smith', 'smoke', 'smooth', 'snake', 'snow', 'soap', 'soapbox', 'soccer', 'social', 'socialist', 'societi', 'sociolog', 'sock', 'sockpuppet', 'sockpuppetri', 'soft', 'softwar', 'soil', 'solar', 'sold', 'soldier', 'sole', 'solicit', 'solid', 'solo', 'solut', 'solv', 'somebodi', 'someday', 'somehow', 'someon', 'someth', 'sometim', 'somewhat', 'somewher', 'son', 'song', 'soni', 'soon', 'sooner', 'sorri', 'sort', 'sought', 'soul', 'sound', 'soundtrack', 'sourc', 'south', 'southern', 'sovereign', 'sovereignti', 'soviet', 'space', 'spade', 'spain', 'spam', 'spammer', 'span', 'spanish', 'spare', 'speak', 'speaker', 'speci', 'special', 'specialist', 'specif', 'specifi', 'spectrum', 'specul', 'speech', 'speed', 'speedi', 'speedili', 'spell', 'spend', 'spent', 'sphere', 'spi', 'spic', 'spider', 'spin', 'spirit', 'spiritu', 'spite', 'split', 'spoiler', 'spoke', 'spoken', 'sponsor', 'sport', 'spot', 'spread', 'spring', 'spurious', 'squad', 'squar', 'sr', 'sri', 'ss', 'st', 'stab', 'stabl', 'stack', 'stadium', 'staff', 'stage', 'stake', 'stalin', 'stalk', 'stalker', 'stamp', 'stanc', 'stand', 'standard', 'star', 'start', 'starter', 'stat', 'state', 'statement', 'static', 'station', 'statist', 'status', 'stay', 'steal', 'steam', 'steel', 'stem', 'step', 'stephen', 'stereotyp', 'steve', 'steven', 'stick', 'stifl', 'still', 'stink', 'stock', 'stolen', 'stone', 'stood', 'stop', 'store', 'stori', 'storm', 'storylin', 'straight', 'straightforward', 'strang', 'strateg', 'strategi', 'straw', 'stream', 'street', 'strength', 'stress', 'stretch', 'strict', 'strike', 'string', 'strip', 'strive', 'stroke', 'strong', 'stronger', 'struck', 'structur', 'struggl', 'stub', 'stubborn', 'stuck', 'student', 'studi', 'studio', 'stuff', 'stumbl', 'stupid', 'style', 'su', 'sub', 'subhead', 'subject', 'submiss', 'submit', 'subpag', 'subscrib', 'subsect', 'subsequ', 'subset', 'subst', 'substanc', 'substant', 'substanti', 'substitut', 'subtl', 'suburb', 'succeed', 'success', 'suck', 'sudden', 'sue', 'suffer', 'suffic', 'suffici', 'suggest', 'suicid', 'suit', 'suitabl', 'sum', 'summar', 'summari', 'summaris', 'summer', 'sun', 'sunday', 'sunni', 'super', 'superior', 'superman', 'superpow', 'supertr', 'suppli', 'support', 'suppos', 'suppress', 'suprem', 'supremacist', 'sure', 'surfac', 'surgeri', 'surnam', 'surpris', 'surround', 'survey', 'surviv', 'survivor', 'suspect', 'suspend', 'suspici', 'suspicion', 'sustain', 'svg', 'swear', 'sweden', 'swedish', 'sweep', 'sweet', 'swift', 'swiss', 'switch', 'sword', 'sydney', 'symbol', 'sympathi', 'symptom', 'syndrom', 'synonym', 'synopsi', 'syntax', 'synth', 'synthesi', 'syria', 'syriac', 'syrian', 'sysop', 'system', 'systemat', 'ta', 'tab', 'tabl', 'tabloid', 'tabtab', 'taco', 'tactic', 'tag', 'tail', 'taiwan', 'tajik', 'tajin', 'take', 'taken', 'tale', 'talent', 'talk', 'talkback', 'talkpag', 'tall', 'tamil', 'tan', 'tank', 'tape', 'target', 'task', 'tast', 'taught', 'tax', 'taylor', 'tc', 'te', 'tea', 'teabag', 'teach', 'teacher', 'teahous', 'team', 'tear', 'tech', 'technic', 'techniqu', 'technolog', 'ted', 'teen', 'teenag', 'teeth', 'teh', 'telephon', 'televis', 'tell', 'telugu', 'temper', 'temperatur', 'templ', 'templat', 'temporari', 'temporarili', 'tempt', 'ten', 'tend', 'tendenc', 'tendenti', 'tenni', 'tens', 'tension', 'term', 'termin', 'terminolog', 'terri', 'terribl', 'territori', 'terror', 'terrorist', 'tertiari', 'test', 'testifi', 'testimoni', 'texa', 'text', 'textbook', 'th', 'thai', 'thank', 'thankyou', 'theater', 'theatr', 'theft', 'theme', 'themselv', 'theolog', 'theorem', 'theoret', 'theori', 'theorist', 'therapi', 'therebi', 'therefor', 'thesi', 'thi', 'thick', 'thin', 'thing', 'think', 'third', 'thirti', 'tho', 'thoma', 'thompson', 'thorough', 'thou', 'though', 'thought', 'thousand', 'thread', 'threat', 'threaten', 'three', 'throughout', 'throw', 'thrown', 'thrust', 'thumb', 'thursday', 'thus', 'thx', 'tibet', 'tibetan', 'ticket', 'tidi', 'tie', 'tiger', 'tight', 'tild', 'till', 'tim', 'time', 'timelin', 'tini', 'tip', 'tire', 'tit', 'titan', 'titl', 'tm', 'today', 'toe', 'togeth', 'told', 'toler', 'tom', 'tomorrow', 'ton', 'tone', 'tongu', 'toni', 'tonight', 'took', 'tool', 'top', 'topic', 'toronto', 'tortur', 'total', 'touch', 'tough', 'tour', 'tournament', 'toward', 'tower', 'town', 'toxic', 'toy', 'tr', 'trace', 'track', 'trade', 'trademark', 'tradit', 'traffic', 'tragedi', 'trail', 'trailer', 'train', 'trait', 'transclud', 'transcript', 'transfer', 'transform', 'transit', 'translat', 'transliter', 'transpar', 'transport', 'trap', 'trash', 'travel', 'treat', 'treati', 'treatment', 'tree', 'trek', 'trend', 'tri', 'trial', 'tribe', 'tribut', 'trick', 'tricki', 'trigger', 'trim', 'trip', 'tripl', 'trivia', 'trivial', 'troll', 'troop', 'tropic', 'troubl', 'truck', 'true', 'truli', 'trump', 'trust', 'truth', 'ts', 'tu', 'tube', 'tuesday', 'tune', 'turk', 'turkey', 'turkic', 'turkish', 'turn', 'tutori', 'tv', 'tw', 'twat', 'tweak', 'twenti', 'twice', 'twin', 'twinkl', 'twist', 'twitter', 'two', 'type', 'typic', 'typo', 'u', 'ufc', 'ugli', 'uh', 'uk', 'ukrain', 'ukrainian', 'ullmann', 'ultim', 'ultra', 'um', 'un', 'unabl', 'unaccept', 'unambigu', 'unawar', 'unbalanc', 'unban', 'unbeliev', 'unbias', 'unblock', 'uncit', 'uncivil', 'uncl', 'unclear', 'unconstruct', 'undelet', 'underground', 'undermin', 'understand', 'understood', 'undertak', 'undid', 'undo', 'undon', 'undoubt', 'undu', 'unemploy', 'unencycloped', 'uneth', 'unfair', 'unfamiliar', 'unfortun', 'unfound', 'unfre', 'unhappi', 'unhelp', 'uniform', 'unilater', 'uninvolv', 'union', 'uniqu', 'unit', 'univers', 'unjust', 'unjustifi', 'unknown', 'unless', 'unlicens', 'unlik', 'unlock', 'unnecessari', 'unnecessarili', 'unoffici', 'unprotect', 'unreason', 'unreferenc', 'unregist', 'unrel', 'unreli', 'unsign', 'unsourc', 'unspecifi', 'unsubscrib', 'unsubstanti', 'unsuccess', 'unsupport', 'unsur', 'untag', 'untru', 'unusu', 'unverifi', 'unwarr', 'unwil', 'upcom', 'updat', 'upgrad', 'uphold', 'upload', 'upon', 'upper', 'upset', 'ur', 'urban', 'urg', 'urgent', 'url', 'us', 'usa', 'usabl', 'usag', 'use', 'useless', 'user', 'userbox', 'usernam', 'userpag', 'userspac', 'ussr', 'usual', 'usurp', 'utc', 'util', 'utter', 'v', 'vacat', 'vaccin', 'vagina', 'vagu', 'valid', 'valley', 'valu', 'valuabl', 'van', 'vancouv', 'vandal', 'vandalis', 'vanish', 'vaniti', 'vari', 'variabl', 'variant', 'variat', 'varieti', 'various', 'vast', 'vatican', 'vector', 'vegan', 'veggietal', 'vehicl', 'vendetta', 'ventur', 'venu', 'verb', 'verbal', 'verbatim', 'veri', 'verif', 'verifi', 'vers', 'version', 'versus', 'vertic', 'vest', 'vet', 'veteran', 'vfd', 'vi', 'via', 'viabl', 'vice', 'vicious', 'victim', 'victor', 'victori', 'victoria', 'video', 'vietnam', 'vietnames', 'view', 'viewer', 'viewpoint', 'vike', 'villag', 'villain', 'violat', 'violenc', 'violent', 'virgin', 'virginia', 'virtual', 'virus', 'visibl', 'vision', 'visit', 'visitor', 'vista', 'visual', 'vital', 'vocal', 'voic', 'vol', 'volum', 'volunt', 'vomit', 'von', 'vote', 'voter', 'vowel', 'vs', 'w', 'wade', 'wage', 'wait', 'wake', 'wale', 'walk', 'walker', 'wall', 'walmart', 'walt', 'walter', 'wan', 'wanker', 'want', 'war', 'ward', 'warfar', 'warm', 'warn', 'warrant', 'warren', 'warrior', 'wash', 'washington', 'wasnt', 'wast', 'watch', 'watchlist', 'water', 'watson', 'wave', 'way', 'wayn', 'weak', 'wealth', 'weapon', 'wear', 'weasel', 'weather', 'web', 'webpag', 'websit', 'wed', 'wee', 'week', 'weekend', 'weigh', 'weight', 'weird', 'welcom', 'well', 'welsh', 'went', 'west', 'western', 'wet', 'whale', 'whatev', 'whatsoev', 'wheel', 'whenev', 'wherea', 'wherev', 'whether', 'whi', 'whilst', 'whine', 'white', 'whitewash', 'whoever', 'whoi', 'whole', 'wholesal', 'wholli', 'whore', 'whose', 'wide', 'wider', 'widespread', 'width', 'wife', 'wiki', 'wikia', 'wikibreak', 'wikifi', 'wikilink', 'wikilov', 'wikimedia', 'wikinew', 'wikipedia', 'wikipedian', 'wikipeida', 'wikiproject', 'wikiquett', 'wikistalk', 'wiktionari', 'wild', 'willi', 'william', 'wilson', 'win', 'wind', 'window', 'wine', 'wing', 'winner', 'winter', 'wipe', 'wire', 'wisconsin', 'wisdom', 'wise', 'wish', 'wit', 'witch', 'withdraw', 'within', 'without', 'wizard', 'wmf', 'wolf', 'woman', 'women', 'wonder', 'wont', 'wood', 'word', 'work', 'worker', 'workshop', 'world', 'worldwid', 'worri', 'wors', 'worship', 'worst', 'worth', 'worthi', 'worthless', 'worthwhil', 'would', 'wound', 'wow', 'wp', 'wrap', 'wrestl', 'wrestler', 'wright', 'write', 'writer', 'written', 'wrong', 'wrote', 'wt', 'wtc', 'wtf', 'ww', 'wwe', 'wwii', 'www', 'x', 'xbox', 'xx', 'ya', 'yaaa', 'yaaaa', 'yadav', 'yahoo', 'yanke', 'ye', 'yea', 'yeah', 'year', 'yellow', 'yep', 'yes', 'yesterday', 'yet', 'yield', 'yo', 'yoga', 'yooo', 'york', 'youbollock', 'young', 'younger', 'yourselfgo', 'yourselv', 'youth', 'youtub', 'yugoslav', 'yugoslavia', 'z', 'zealand', 'zero', 'zionist', 'zone']\n"
     ]
    }
   ],
   "source": [
    "print(feature_name_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cv_os_all = []\n",
    "y_train_cv_os_all = []\n",
    "\n",
    "x_train_tfidf_os_all = []\n",
    "y_train_tfidf_os_all = []\n",
    "\n",
    "x_train_ngram_os_all = []\n",
    "y_train_ngram_os_all = []\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    sm_cv = SMOTE(random_state=40)\n",
    "    x_train_cv_os, y_train_cv_os = sm_cv.fit_resample(x_train_cv, y_train.iloc[:,i])\n",
    "    x_train_cv_os_all.append(x_train_cv_os)\n",
    "    y_train_cv_os_all.append(y_train_cv_os)\n",
    "    \n",
    "    sm_tfidf = SMOTE(random_state=40)\n",
    "    x_train_tfidf_os, y_train_tfidf_os = sm_tfidf.fit_resample(x_train_tfidf, y_train.iloc[:,i])\n",
    "    x_train_tfidf_os_all.append(x_train_tfidf_os)\n",
    "    y_train_tfidf_os_all.append(y_train_tfidf_os)\n",
    "    \n",
    "    sm_ngram = SMOTE(random_state=40)\n",
    "    x_train_ngram_os, y_train_ngram_os = sm_ngram.fit_resample(x_train_tfidf_ngram, y_train.iloc[:,i])\n",
    "    x_train_ngram_os_all.append(x_train_ngram_os)\n",
    "    y_train_ngram_os_all.append(y_train_ngram_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_y_train_all = [x_train_cv_os_all, y_train_cv_os_all, x_train_tfidf_os_all, y_train_tfidf_os_all, x_train_ngram_os_all, y_train_ngram_os_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_y_test_all = [x_test_cv, y_test, x_test_tfidf, y_test, x_test_tfidf_ngram, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201698, 5000)\n",
      "(221074, 5000)\n",
      "(211334, 5000)\n",
      "(222628, 5000)\n",
      "(212120, 5000)\n",
      "(221320, 5000)\n"
     ]
    }
   ],
   "source": [
    "for i in x_train_cv_os_all:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# where do I want to store this file?\n",
    "# Open the file to save as pkl file\n",
    "train_data_path = 'train_data_array.pkl'\n",
    "train_data_path_pkl = open(train_data_path, 'wb')\n",
    "pickle.dump(x_train_y_train_all, train_data_path_pkl)\n",
    "\n",
    "# Close the pickle instances\n",
    "train_data_path_pkl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = 'test_data_array.pkl'\n",
    "test_data_path_pkl = open(test_data_path, 'wb')\n",
    "pickle.dump(x_test_y_test_all, test_data_path_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Feature Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:08:17.306241Z",
     "start_time": "2018-11-10T21:08:16.261060Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "x_train_y_train_all_load = pickle.load(open('train_data_array.pkl', 'rb'))\n",
    "x_test_y_test_all_load = pickle.load(open('test_data_array.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:08:18.234412Z",
     "start_time": "2018-11-10T21:08:18.225172Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_cv_os_all = x_train_y_train_all_load[0]\n",
    "y_train_cv_os_all = x_train_y_train_all_load[1]\n",
    "\n",
    "x_train_tfidf_os_all = x_train_y_train_all_load[2]\n",
    "y_train_tfidf_os_all = x_train_y_train_all_load[3]\n",
    "\n",
    "x_train_ngram_os_all = x_train_y_train_all_load[4]\n",
    "y_train_ngram_os_all = x_train_y_train_all_load[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:08:23.090715Z",
     "start_time": "2018-11-10T21:08:23.075766Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test_cv = x_test_y_test_all_load[0]\n",
    "x_test_tfidf = x_test_y_test_all_load[2]\n",
    "x_test_tfidf_ngram = x_test_y_test_all_load[4]\n",
    "y_test = x_test_y_test_all_load[1]\n",
    "\n",
    "y_test = [np.array(y_test.iloc[:,i]).reshape(-1,1) for i in range(6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vector Feature Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T12:31:38.796008Z",
     "start_time": "2018-11-03T12:31:38.784106Z"
    }
   },
   "outputs": [],
   "source": [
    "class toxicmodel:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, n = 6):\n",
    "        self.n = n\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        self.best_params = []\n",
    "        self.best_estimator = []\n",
    "        \n",
    "        self.y_predict_train = []\n",
    "        self.y_predict_test = []\n",
    "        self.y_predict_proba_train = []\n",
    "        self.y_predict_proba_test = []\n",
    "\n",
    "        self.acc_score_train = []\n",
    "        self.acc_score_test = []\n",
    "\n",
    "        self.roc_auc_score_train = []\n",
    "        self.roc_auc_score_test = []\n",
    "\n",
    "        self.f1_score_train = []\n",
    "        self.f1_score_test = []\n",
    "\n",
    "        self.confusion_matrix_train = []\n",
    "        self.confusion_matrix_test = []\n",
    "\n",
    "        self.classification_report_train = []\n",
    "        self.classification_report_test = []\n",
    "\n",
    "    \n",
    "    def trainmodel(self, model_name, hyper_param_grid):\n",
    "        for i in range(self.n):\n",
    "            grid_search_model = GridSearchCV(model_name, hyper_param_grid, scoring = 'f1', cv = 5,refit = True, n_jobs=-1, verbose = 5)\n",
    "            grid_search_model.fit(self.x_train[i], self.y_train[i])\n",
    "            self.best_params.append(grid_search_model.best_params_)\n",
    "            self.best_estimator.append(grid_search_model.best_estimator_)\n",
    "    \n",
    "    \n",
    "    def predictmodel(self):\n",
    "        for i in range(self.n):\n",
    "            \n",
    "            y_predict_train = self.best_estimator[i].predict(self.x_train[i])\n",
    "            y_predict_test = self.best_estimator[i].predict(self.x_test)\n",
    "             \n",
    "            #y_predict_proba_train = self.best_estimator[i].predict_proba(self.x_train[i])[:,1]\n",
    "            #y_predict_proba_test = self.best_estimator[i].predict_proba(self.x_test)[:,1]\n",
    "            \n",
    "\n",
    "            #self.y_predict_train.append(y_predict_train)\n",
    "            #self.y_predict_test.append(y_predict_test)\n",
    "            \n",
    "            #self.y_predict_proba_train.append(y_predict_proba_train)\n",
    "            #self.y_predict_proba_test.append(y_predict_proba_test)\n",
    "\n",
    "            #self.roc_auc_score_train.append(roc_auc_score(self.y_train[i], y_predict_proba_train))\n",
    "            #self.roc_auc_score_test.append(roc_auc_score(self.y_test[i], y_predict_proba_test))\n",
    "            \n",
    "            self.acc_score_train.append(accuracy_score(self.y_train[i], y_predict_train))\n",
    "            self.acc_score_test.append(accuracy_score(self.y_test[i], y_predict_test))\n",
    "            \n",
    "            self.f1_score_train.append(f1_score(self.y_train[i], y_predict_train))\n",
    "            self.f1_score_test.append(f1_score(self.y_test[i], y_predict_test))\n",
    "\n",
    "            self.confusion_matrix_train.append(confusion_matrix(self.y_train[i], y_predict_train))\n",
    "            self.confusion_matrix_test.append(confusion_matrix(self.y_test[i], y_predict_test))\n",
    "\n",
    "            self.classification_report_train.append(classification_report(self.y_train[i], y_predict_train))\n",
    "            self.classification_report_test.append(classification_report(self.y_test[i], y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T11:26:40.938407Z",
     "start_time": "2018-11-03T11:24:29.774673Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   11.7s remaining:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   13.4s finished\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   11.6s remaining:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   12.5s finished\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   10.1s remaining:   15.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   11.9s finished\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   13.2s remaining:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   13.8s finished\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   11.0s remaining:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   12.5s finished\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   13.2s remaining:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   13.8s finished\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/linear_model/base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/home/targoon/.local/lib/python3.6/site-packages/sklearn/linear_model/base.py:297: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    log_toxic = toxicmodel(x_train_cv_os_all, y_train_cv_os_all, x_test_cv, y_test)\n",
    "    log_toxic.trainmodel(LogisticRegression(), {'random_state':[0]})\n",
    "    log_toxic.predictmodel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-03T12:41:15.113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "svml_toxic = toxicmodel(x_train_cv_os_all, y_train_cv_os_all, x_test_cv, y_test, n = 1)\n",
    "svml_toxic.trainmodel(SVC(kernel='linear', probability=True), {'C':np.arange(0.01,0.1,0.02)})\n",
    "svml_toxic.predictmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svml_toxic = toxicmodel(n=1, x_train = x_train_cv_os_all,y_train=y_train_cv_os_all,x_test=x_test_cv, y_test=y_test)\n",
    "svml_toxic.trainmodel(RandomForestClassifier(), {'n_estimators':[500, 750, 1000],'max_features':[10, 25, 40, 65],'random_state':[0], 'max_depth':[4,6,8]})\n",
    "svml_toxic.predictmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T12:36:16.865264Z",
     "start_time": "2018-11-03T12:35:36.643Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svml_toxic.best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T12:36:16.868042Z",
     "start_time": "2018-11-03T12:35:45.882Z"
    }
   },
   "outputs": [],
   "source": [
    "svml_toxic.acc_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T12:36:16.870669Z",
     "start_time": "2018-11-03T12:35:48.755Z"
    }
   },
   "outputs": [],
   "source": [
    "svml_toxic.acc_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svml_toxic.acc_score_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:38:46.651935Z",
     "start_time": "2018-11-10T21:38:46.640281Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:33:59.787915Z",
     "start_time": "2018-11-10T21:33:59.578615Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(5000,)))\n",
    "model.add(Activation('relu')) \n",
    " \n",
    "# Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
    "model.add(Dropout(0.2))   \n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:34:03.251014Z",
     "start_time": "2018-11-10T21:34:03.237220Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               2560512   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,824,194\n",
      "Trainable params: 2,824,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:34:08.703288Z",
     "start_time": "2018-11-10T21:34:08.616422Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:39:42.534979Z",
     "start_time": "2018-11-10T21:39:42.516330Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train_cv_os_all[0], 2)\n",
    "y_test = np_utils.to_categorical(y_test[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:47:44.232376Z",
     "start_time": "2018-11-10T21:39:44.257993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 201698 samples, validate on 47857 samples\n",
      "Epoch 1/20\n",
      "201698/201698 [==============================] - 178s 883us/step - loss: 0.1671 - val_loss: 0.1771\n",
      "Epoch 2/20\n",
      "201698/201698 [==============================] - 161s 799us/step - loss: 0.0924 - val_loss: 0.2066\n",
      "Epoch 3/20\n",
      "180736/201698 [=========================>....] - ETA: 16s - loss: 0.0551"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b2ff71857db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           validation_data=(x_test_cv, y_test))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_cv_os_all[0], y_train,\n",
    "          batch_size=128, epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test_cv, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Preprocessing.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
